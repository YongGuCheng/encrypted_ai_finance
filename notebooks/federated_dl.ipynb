{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0827 22:01:59.621158 139673054320448 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/home/mkucz/p_venv/lib/python3.6/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0827 22:01:59.629590 139673054320448 deprecation_wrapper.py:119] From /home/mkucz/p_venv/lib/python3.6/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import syft as sy\n",
    "import torch as th\n",
    "from helpers import Model\n",
    "\n",
    "# BEWARE, ignoreing warnings is not always a good idea\n",
    "# I am doing it for presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"federated_dl\"></a>\n",
    "## Federated Deep Learning\n",
    "The idea behind federated learning is that we train a model on subsets of data (encrypted or otherwise) that never leaves the ownership of an individual. In this example of credit rating scores it would allow people to submit claims without ever losing ownership of their data. It requires very little trust of the party to which the application is being submitted.\n",
    "\n",
    "Even though we currently have our dataset located locally, we want to simulate having many people in our network who each maintain ownership of their data. Therefore we have to create a virtual worker for each datum. The work/data flow in this situation would be as follows:\n",
    "\n",
    "- get pointers to training data on each remote worker <br>\n",
    "**Training Steps:**\n",
    "- send model to remote worker\n",
    "- train model on data located with remote worker\n",
    "- receive updated model from remote worker\n",
    "- repeat for all workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load('../data/features.npy')\n",
    "labels = np.load('../data/labels_dim.npy')\n",
    "data = th.tensor(features, dtype=th.float32, requires_grad=True)\n",
    "target = th.tensor(labels, dtype=th.float32, requires_grad=False).reshape(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self, in_size, out_size, hidden_layers,\n",
    "                       activation=F.softmax, dim=-1):\n",
    "        self.batch_size = 1\n",
    "        self.drop_p = None\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.001\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.precision_fractional=10\n",
    "        self.activation = activation\n",
    "        self.dim = dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(th)\n",
    "\n",
    "def connect_to_workers(n_workers, secure_worker=False):\n",
    "    '''\n",
    "    Connect to remote workers with PySyft\n",
    "    \n",
    "    Inputs\n",
    "        n_workers (int) - how many workers to connect to\n",
    "        secure_worker (bool) - whether to return a trusted aggregator as well\n",
    "        \n",
    "    Outputs\n",
    "        workers (list, <PySyft.VirtualWorker>)\n",
    "    '''\n",
    "    workers = [sy.VirtualWorker(hook, id=f'w_{i}') for i in range(n_workers)]\n",
    "\n",
    "    if secure_worker:\n",
    "        return workers, sy.VirtualWorker(hook, id='trusted_aggregator')\n",
    "\n",
    "    else:\n",
    "        return workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = th.load('base_model.pt') #use model trained earlier to save time\n",
    "\n",
    "dataset = [(data[i], target[i]) for i in range(len(data))]\n",
    "\n",
    "#instantiate model\n",
    "in_size = checkpoint['in_size']\n",
    "out_size = checkpoint['out_size']\n",
    "hidden_layers=checkpoint['hidden_layers']\n",
    "\n",
    "workers = connect_to_workers(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<VirtualWorker id:w_0 #objects:0>,\n",
       " <VirtualWorker id:w_1 #objects:0>,\n",
       " <VirtualWorker id:w_2 #objects:0>,\n",
       " <VirtualWorker id:w_3 #objects:0>,\n",
       " <VirtualWorker id:w_4 #objects:0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers[:5] \n",
    "# each individual worker corresponds to a person, or rather their device\n",
    "# currently these people have no objects associated with them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Data to Remote Worker\n",
    "In reality the data of each person would already be on a remote worker. Either each person's device or aggregated into multiple remote workers by a secure third party.\n",
    "\n",
    "Here we have two options:\n",
    "1. send the data to each worker individually\n",
    "2. use PySyft's implementation of PyTorch's `Dataset` and `DataLoader`\n",
    "\n",
    "I will use PySyft's `BaseDataset`, `FederatedDataset` and `FederatedDataLoader` since this simplifies dataprocessing for larger applications, even though it is not necessary for this example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1\n",
    "```\n",
    "remote_dataset = []\n",
    "for i in range(len(dataset)):\n",
    "    d, t = dataset[i]\n",
    "    \n",
    "    r_d = d.reshape(1,-1).send(workers[i])\n",
    "    r_t = t.reshape(1,-1).send(workers[i])\n",
    "    \n",
    "    remote_dataset.append((r_d, r_t))\n",
    "    \n",
    "r_d, r_t = remote_dataset[0]\n",
    "r_d #this is now a pointer to remote data rather than an actual tensor on our device\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w_0', 'w_1', 'w_2', 'w_3', 'w_4']\n"
     ]
    }
   ],
   "source": [
    "# Option 2\n",
    "# Cast the result in BaseDatasets\n",
    "remote_dataset_list = []\n",
    "for i in range(len(dataset)):\n",
    "    d, t = dataset[i] #get data\n",
    "\n",
    "    #send to worker before adding to dataset\n",
    "    r_d = d.reshape(1,-1).send(workers[i])\n",
    "    r_t = t.reshape(1,-1).send(workers[i])\n",
    "    \n",
    "    dtset = sy.BaseDataset(r_d, r_t)\n",
    "    remote_dataset_list.append(dtset)\n",
    "\n",
    "# Build the FederatedDataset object\n",
    "remote_dataset = sy.FederatedDataset(remote_dataset_list)\n",
    "print(remote_dataset.workers[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_dataset_list[0].targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<VirtualWorker id:w_0 #objects:2>,\n",
       " <VirtualWorker id:w_1 #objects:2>,\n",
       " <VirtualWorker id:w_2 #objects:2>,\n",
       " <VirtualWorker id:w_3 #objects:2>,\n",
       " <VirtualWorker id:w_4 #objects:2>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = sy.FederatedDataLoader(remote_dataset, batch_size=1,\n",
    "                                      shuffle=False, drop_last=False)\n",
    "\n",
    "workers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# new training logic to reflect federated learning\n",
    "def federated_train(model, datasets, criterion, args):\n",
    "    # use a simple stochastic gradient descent optimizer\n",
    "    # define optimizer for each model\n",
    "    optimizer = optim.SGD(params=model.parameters(), lr=args.lr)\n",
    "\n",
    "    print(f'Federated Training on {len(datasets)} remote workers (dataowners)')\n",
    "    steps = 0\n",
    "    model.train()  # training mode\n",
    "\n",
    "    for e in range(1, args.epochs+1):\n",
    "        running_loss = 0\n",
    "        for ii, (data, target) in enumerate(datasets):\n",
    "            # iterates over pointers to remote data\n",
    "            steps += 1\n",
    "\n",
    "            # FEDERATION STEP\n",
    "            model.send(data.location)\n",
    "            # send model to remote worker\n",
    "\n",
    "            # NB the steps below all happen remotely\n",
    "            # zero out gradients so that one forward pass\n",
    "            # doesnt pick up previous forward's gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model.forward(data)  # make prediction\n",
    "            # get shape of (1,2) as we need at least two dimension\n",
    "            outputs = outputs.reshape(1, -1)\n",
    "\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # FEDERATION STEP\n",
    "            model.get()  # get model with new gradients back from remote worker\n",
    "\n",
    "            # FEDERATION STEP\n",
    "            l = loss.get()\n",
    "            running_loss += l  # get loss from remote worker\n",
    "        print('Train Epoch: {} \\tLoss: {:.6f}'.format(e, running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated Training on 653 remote workers (dataowners)\n",
      "Train Epoch: 1 \tLoss: 56.347801\n",
      "Train Epoch: 2 \tLoss: 55.781364\n",
      "Train Epoch: 3 \tLoss: 55.508972\n",
      "Train Epoch: 4 \tLoss: 55.245068\n",
      "Train Epoch: 5 \tLoss: 55.010921\n",
      "Train Epoch: 6 \tLoss: 54.805840\n",
      "Train Epoch: 7 \tLoss: 54.632221\n",
      "Train Epoch: 8 \tLoss: 54.474529\n",
      "Train Epoch: 9 \tLoss: 54.311638\n",
      "Train Epoch: 10 \tLoss: 54.210529\n",
      "CPU times: user 1min 15s, sys: 177 ms, total: 1min 15s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "args = Arguments(in_size, out_size, hidden_layers, \n",
    "                 activation=F.softmax, dim=1)\n",
    "model = Model(args)\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "federated_train(model, train_loader, nn.MSELoss(), args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Viola!_ Now we have a federated model where the data never leaves the ownership of a remote device. We can implement this in a way where each user's device is a worker. The problem that occurs here, is that even though the data never leaves an owner's device, `model.get()` returns a new version of the model, which in turn violates privacy of the data owners by revealing information on their data through the updates that were made to the model. A solution to this problem is to use a **trusted third-party aggregator** to combine the remotely trained models into one, *before* sending it to the end-user (in this case me, the credit provider).\n",
    "\n",
    "Notice how the federated model is about 32x slower than the non-federated model. This is simply one of the trade-offs that we have to be willing to make.\n",
    "\n",
    "And to be honest, I'm not quite sure why the loss starts at 55... this is overall the same model and implementation as the vanilla neural network I implemented earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in this journey is **Federated Learning with Model Averaging** which you can find [here]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.183px",
    "left": "910px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
