{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Privacy - DP\n",
    "\n",
    "### What is DP?\n",
    "\n",
    "Differential Privacy began with ensuring that different *'statistical analysis'* does not violate privacy, which in the early days of DP meant database queries that remained private. Now, any statistical analysis should not violate the privacy of any individual.\n",
    "\n",
    "\n",
    "We want to learn information about a dataset but not about specific individuals, knowledge of which could harm them. We need an enforceable definition of privacy so that we can evaluate our practices.\n",
    "**Definition** <br> *Differential Privacy* describes a promise, made by a data holder, or curator, to a data subject, and the promise is as follows: <br> \"You will not be affected, adversely or otherwise, by allowing your data to be used in any study or analysis, no matter what other studies, data sets, or information sources, are available.\" -- Cynthia Dwork, \"*Algorithmic Foundations*\"\n",
    "\n",
    "Simply anonymizing data does not work. This would violate promise of our definitions where we assert that we will not affect anyone in any way through the release of data. Often times data can be deduced from even an anonymized dataset by studying a separate dataset. For example, the *Netflix Prize* competition included a release of many movie ratings by many users. Even though this data was fully anonymized, [statistical analysis allowed researchers to de-anonymize the user's names as well as movie titles](https://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf). A simple Google search reveals many more instances of de-anonymization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Example\n",
    "Consider a database that is equivalent to a binary array. We want to perform a query such that regardless of the query we make, removing a datum from the database the query does not change. This is an example of full privacy protection. This datum was not leaking any statistical information into the database. <br> When we want to evaluate the privacy of a function (or query), we want to compare the output of the query on the entire database against the query on each parallel database. A parallel database is a database with one datum removed. See Andrew Trask's work for this example [here](https://github.com/udacity/private-ai/blob/master/completed/Section%201%20-%20Differential%20Privacy.ipynb).\n",
    "\n",
    "\n",
    "**Differencing Attack**\n",
    "If we are able to perform a one or more queries and gain a single datum's information or data, we are able to circumvent privacy protections and divulge specific data. For example comparing various sum queries to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local vs Global DP\n",
    "Main strategy to protect data, we will be adding random noise. \n",
    "- *Local Differential Privacy*: adds noise to function data points (function inputs)\n",
    "    - examples of this would be adding noise directly to a database or have users add noise to their own data, before it is even put in the DB. This offers highest level of protection since users do not need to trust anyone before handing over data.\n",
    "- *Global Differential Privacy*: adds noise to function outputs\n",
    "    - A database contains private information, but we add noise to interfaces with data.\n",
    "    \n",
    "Assuming that a database operator is trustworthy, the largest difference between local and global differential privacy is that under the global DP scenario our results will be more accurate. This assumption relies on a *'trusted curator'* to add noise in a proper manner.\n",
    "\n",
    "### Randomized Response\n",
    "Randomized response is a great example of how we can make functions DP. Read about it [here](https://en.wikipedia.org/wiki/Randomized_response). In essence a question posed by a researcher adds plausible deniability to tease out the truth over a large population after adjusting for the true statistic.\n",
    "\n",
    "## Formal Definition of Differential Privacy\n",
    "There are two types of noise we can add, *Gaussian* or *Laplacian* noise (based on the respective distributions). <br> How much noise should we add? We can analyze a query against our definition of DP. But first, let's formally define DP.\n",
    "\n",
    "**Definition**<br>\n",
    "For ALL parallel databases, the maximum distance between a query on database (*x*) and the same query on database (*y*) will be $e^\\varepsilon$, but that occasionally this constraint won't hold with probability delta. Thus, this theorem is called *'epsilon delta'* differential privacy\n",
    "\n",
    "In essence the equation in the definition compares two random distributions (from randomized algorithm *M*) over all things in DB, where one random distribution has one item missing. We want to see how different these two distributions are.\n",
    "\n",
    "To read more about the mathematics behind $\\varepsilon$-differential privacy go [here](https://en.wikipedia.org/wiki/Differential_privacy#Definition_of_%CE%B5-differential_privacy).\n",
    "\n",
    "\n",
    "*Epsilon*: $ e^{\\varepsilon}$, is the primary constraint on maximum amount that the two distributions are different\n",
    "   - $\\varepsilon=0$ -> PERFECT PRIVACY (with $\\delta==0$)\n",
    "   - $\\varepsilon=1$ -> allows some privacy leakage.\n",
    "Note, our constraint can still allow for leakage of small amounts.\n",
    "\n",
    "*Delta*: $\\delta$ is the probability that we leak more information than $\\varepsilon$ claims we leak. We want this to be small number.\n",
    "\n",
    "**Privacy Budget**\n",
    "How much epsilon/delta leakage we allow for our analysis. \n",
    "Laplacian noise works best.But how much do we add? It depends on the type of noise we are adding. Take into account a query's sensitivity, and our desired epsilon, delta budget.<br>\n",
    "\n",
    "**Laplacian noise** <br>\n",
    "$b = \\frac{sensitivity(query)}{\\varepsilon}$ (where $\\delta=0$ always)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differencial Privacy for Deep Learning\n",
    "\n",
    "Differential privacy techniques form the basis of privacy guarantees in the context of deep learning. In the context of DL, we can replace our 'query' terminology with 'neural network training'. Regardless of any single datum we remove from our database, we should always get the same model back. This becomes tricky with DL since it is common for models to train to different solutions (different outputs) even on the same data. Furthermore, we might not always know where individual people are referenced in a database. Sometimes we can have multiple people per datum, like with images of multiple people. Defining DP in this context is difficult.\n",
    "\n",
    "Generally, there are three ways of making deep learning differentially private:\n",
    "1. Add *noise to gradients* through differentially private stochastic gradient descent (local differential privacy)\n",
    "2. Add *noise to input data* (local differential privacy)\n",
    "3. Add *noise to predictions* of a model (i.e. PATE)\n",
    "\n",
    "### PATE Analysis\n",
    "[*Private Aggregation of Teacher Ensembles* (PATE) Analysis](https://arxiv.org/pdf/1802.08908.pdf) is a proposed solution for assessing a DL model's DP.\n",
    "In essence, it works by performing a DP query (usually a max query that we make DP with an epsilon delta constraint) on the predictions of remotely trained models on a new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "For more information on differential privacy, head to an example by Andrew Trask [here](https://github.com/udacity/private-ai/blob/master/completed/Section%201%20-%20Differential%20Privacy.ipynb), or read the most comprehensive differential privacy textbook to date, [\"Algorithmic Foundations of Differential Privacy\"](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf) by Cynthia Dwork and Aaron Roth.\n",
    "\n",
    "##### References\n",
    "Subject matter and code inspired by Udacity's Private AI Scholarship Challenge\n",
    "- https://github.com/udacity/private-ai/blob/master/completed/Section%201%20-%20Differential%20Privacy.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
