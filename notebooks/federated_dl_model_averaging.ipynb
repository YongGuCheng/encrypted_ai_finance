{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0828 20:18:32.488130 139805837973312 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/home/mkucz/p_venv/lib/python3.6/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0828 20:18:32.496996 139805837973312 deprecation_wrapper.py:119] From /home/mkucz/p_venv/lib/python3.6/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import syft as sy\n",
    "import torch as th\n",
    "from helpers import Model, connect_to_workers\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "# BEWARE, ignoreing warnings is not always a good idea\n",
    "# I am doing it for presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load('../data/features.npy')\n",
    "labels = np.load('../data/labels_dim.npy')\n",
    "data = th.tensor(features, dtype=th.float32, requires_grad=True)\n",
    "target = th.tensor(labels, dtype=th.float32, requires_grad=False).reshape(-1,2)\n",
    "\n",
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self, in_size, out_size, hidden_layers,\n",
    "                       activation=F.softmax, dim=-1):\n",
    "        self.batch_size = 1\n",
    "        self.drop_p = None\n",
    "        self.epochs = 1\n",
    "        self.lr = 0.001\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.precision_fractional=10\n",
    "        self.activation = activation\n",
    "        self.dim = dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = th.load('base_model.pt')  # use model trained earlier to save time\n",
    "\n",
    "dataset = [(data[i], target[i]) for i in range(len(data))]\n",
    "\n",
    "# instantiate model\n",
    "in_size = checkpoint['in_size']\n",
    "out_size = checkpoint['out_size']\n",
    "hidden_layers = checkpoint['hidden_layers']\n",
    "\n",
    "# for MSE loss, we want to use softmax and not log_softmax\n",
    "args = Arguments(in_size, out_size, hidden_layers,\n",
    "                 activation=F.softmax, dim=1)\n",
    "# PyTorch's softmax activation only works with floats\n",
    "workers, trusted_aggregator = connect_to_workers(len(dataset), hook, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fl_model_avg\"></a>\n",
    "### Federated Learning with Model Averaging\n",
    "We can perform federated learning in a way that trains a model on the data of each remote worker, and uses a *'trusted aggregator'* to combine the models into one. In this way, the non-trusted party, me for example, cannot tell which remote worker has updated gradients in what way. Gradient updates can be reverse engineered to understand what data has been passed through the network. This is an added layer of privacy protection in federated learning. The downside of this approach, however, is that it requires all parties to trust said aggregator.\n",
    "\n",
    "A couple of quick things to note before starting this notebook, is that based on the way model averaging is performed, we cannot use [NLLLoss](https://pytorch.org/docs/stable/nn.html#nllloss), and therefore we are using [MSELoss](https://pytorch.org/docs/stable/nn.html#mseloss). This also means that we have to get the labels in a little differently (one-hot-encoded labels versus just the label). This also has an implication in terms of the activation function (if any) that we want to use. Instead of using [LogSoftmax](https://pytorch.org/docs/stable/nn.html#logsoftmax) which returns the log of the softmax function output, we would want to use the normal [Softmax](https://pytorch.org/docs/stable/nn.html#softmax)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send Data to Remote Worker\n",
    "In this step we need to send a copy of the model to each remote worker, as well as a new optimizer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w_0', 'w_1', 'w_2', 'w_3', 'w_4']\n"
     ]
    }
   ],
   "source": [
    "# Send data to remote workers\n",
    "# Cast the result in BaseDatasets\n",
    "remote_dataset_list = []\n",
    "for i in range(len(dataset)):\n",
    "\n",
    "    d, t = data[i], target[i]\n",
    "    # send to worker before adding to dataset\n",
    "    r_d = d.reshape(1, -1).send(workers[i])\n",
    "    r_t = t.reshape(1, -1).send(workers[i])\n",
    "\n",
    "    dtset = sy.BaseDataset(r_d, r_t)\n",
    "    remote_dataset_list.append(dtset)\n",
    "\n",
    "# Build the FederatedDataset object\n",
    "n_train_items = int(len(dataset)*0.7)\n",
    "n_test_items = len(dataset) - n_train_items\n",
    "# split into train/test\n",
    "train_remote_dataset = sy.FederatedDataset(remote_dataset_list[:n_train_items])\n",
    "test_remote_dataset = sy.FederatedDataset(remote_dataset_list[n_train_items:])\n",
    "\n",
    "print(train_remote_dataset.workers[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(args)\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "#send copy of model to remote client worker\n",
    "models = [model.copy().send(w) for w in workers] \n",
    "optimizers = [optim.SGD(params=m.parameters(), lr=args.lr) for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARALLEL federated learning with trusted aggregator\n",
    "def federated_train_trusted_agg(models, datasets, optimizers):\n",
    "    for e in range(1, args.epochs+1):\n",
    "        running_loss = 0\n",
    "        for i in range(n_train_items):  # train each model concurrently\n",
    "            model = models[i] # choose remote model to use\n",
    "            opt = optimizers[i] #choose remote optimizer to use\n",
    "            \n",
    "            # get remote dataset loc (by worker id)\n",
    "            _d = datasets.datasets[model.location.id]  \n",
    "\n",
    "            # NB the steps below all happen remotely\n",
    "            opt.zero_grad()  \n",
    "            # zero out gradients so that one forward pass\n",
    "            # doesnt pick up previous forward's gradients\n",
    "            outputs = model.forward(_d.data)  # make prediction\n",
    "            # get shape of (1,2) as we need at least two dimension\n",
    "            outputs = outputs.reshape(1, -1)\n",
    "            \n",
    "            # NllLoss does not work well with federation...\n",
    "            loss = ((outputs - _d.targets)**2).sum()\n",
    "            \n",
    "            #or\n",
    "            #loss = F.mse_loss(outputs, _d.targets)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # FEDERATION STEP\n",
    "            _loss = loss.get().data  # get loss from remote worker\n",
    "            if th.isnan(_loss) or _loss > 10:\n",
    "                print(model.location.id, outputs.get(), _d.targets.get(), _loss)\n",
    "                continue\n",
    "\n",
    "            running_loss += _loss\n",
    "        print('Epoch: {} \\tLoss: {:.6f}'.format(\n",
    "            e, running_loss/i))\n",
    "\n",
    "    # move trained models to trusted thrid party\n",
    "    for m in models:\n",
    "        m.move(trusted_aggregator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 0.142975\n"
     ]
    }
   ],
   "source": [
    "federated_train_trusted_agg(models, train_remote_dataset, optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have parallel training implemented, we want to add logic that averages the models of each remote worker after each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_avg(base_model, models):\n",
    "    '''\n",
    "    Average weights and biases of models on trusted aggregator\n",
    "\n",
    "    Parameters\n",
    "    ::models - list: pointers to remote models, should be on trusted aggregator\n",
    "\n",
    "    Returns\n",
    "    ::avg_weights\n",
    "    ::avg_bias\n",
    "    '''\n",
    "    # average out each hidden layer individually\n",
    "    for i in range(len(base_model.hidden_layers)):\n",
    "        weights, biases = zip(*[(m.hidden_layers[i].weight.data,\n",
    "                                 m.hidden_layers[i].bias.data) for m in models])\n",
    "        base_model.hidden_layers[i].weight\\\n",
    "                                   .set_((sum(weights)/len(models)).get())\n",
    "        base_model.hidden_layers[i].bias.set_((sum(biases)/len(models)).get())\n",
    "\n",
    "    # average out output layer\n",
    "    weights, biases = zip(*[(m.output.weight.data,\n",
    "                             m.output.bias.data) for m in models])\n",
    "    base_model.output.weight.set_((sum(weights)/len(models)).get())\n",
    "    base_model.output.bias.set_((sum(biases)/len(models)).get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the model on trusted aggregator\n",
    "with th.no_grad():\n",
    "    set_model_avg(model, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it Together\n",
    "Now put together the training and averaging step into one, where the overall model is averaged on a trusted aggregator after every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated Training \n",
      " 653 remote workers\n",
      " 1 Trusted Aggregator\n",
      " 653 datum\n",
      "Epoch: 1 \tLoss: 0.177108\n",
      "CPU times: user 9.56 s, sys: 127 ms, total: 9.68 s\n",
      "Wall time: 9.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print((f'Federated Training \\n {len(workers)} remote workers'\n",
    "       f'\\n {len(remote_dataset)} datum'\n",
    "        '\\n 1 Trusted Aggregator'))\n",
    "model = Model(args)\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "for i in range(1, args.epochs+1):\n",
    "    models = [model.copy().send(w) for w in workers]\n",
    "    optimizers = [optim.SGD(params=m.parameters(), lr=args.lr) for m in models]\n",
    "\n",
    "    federated_train_trusted_agg(models, train_remote_dataset, optimizers)\n",
    "\n",
    "    # Average the model on trusted aggregator\n",
    "    with th.no_grad():\n",
    "        set_model_avg(model, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now trained a deep learning model using federated learning with a trusted aggregator! Make sure to test the model on a hold-out dataset. For the purpose of these examples, I will exclude testing sets for the sake of time.\n",
    "Nevertheless, this **data is not yet encrypted** and we could deduce things specific to the applicant just by getting or looking at the remote data. <br>\n",
    "In comes **encrypted deep learning**! Here we want to encrypt gradients such that no trusted aggregator is needed! To check out this exciting code [click here](https://github.com/mkucz95/private_ai_finance#encrypted-deep-learning).\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.183px",
    "left": "910px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
