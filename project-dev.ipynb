{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Private and Ecrypted AI - Credit Approval Application\n",
    "1. <a href=\"#data_prep\">Data Preparation & Setup</a>\n",
    "2. <a href=\"#classical_dl\">Classical Deep Learning</a>\n",
    "3. Federated Deep Learning\n",
    "    - Secured Multi-Party Computation (SMPC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_prep'></a>\n",
    "## Data Preparation\n",
    "- only using non-NaN values. I drop NaN values because the dataset is not very big regardless, and we are not dropping very many values.\n",
    "- Convert binary variables to a numeric representation, and one-hot-encode categorical variables. We do not want to use label encoder since a label encoder would make it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [ f\"A{i}\" for i in range(1,16)]\n",
    "cols.append('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(653, 16) \n",
      " ------- \n",
      "\n",
      "  A1     A2    A3 A4 A5 A6 A7    A8 A9 A10  A11 A12 A13    A14  A15 label\n",
      "0  b  30.83  0.00  u  g  w  v  1.25  t   t    1   f   g  00202    0     +\n",
      "1  a  58.67  4.46  u  g  q  h  3.04  t   t    6   f   g  00043  560     +\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/crx.data', names=cols)\\\n",
    "    .replace(to_replace='?', value=np.nan).dropna()\n",
    "print(df.shape, \"\\n ------- \\n\")\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary(df, col):\n",
    "    u = df[col].unique()\n",
    "    mapping =dict(zip(u, [i for i in range(0,len(u))]))\n",
    "    return df[col].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b\n",
       "1    a\n",
       "2    a\n",
       "3    b\n",
       "4    b\n",
       "Name: A1, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.A1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to float\n",
    "for col in ['A2', 'A3', 'A8', 'A11', 'A14', 'A15']:\n",
    "    df[col] = df[col].astype(float)\n",
    "    \n",
    "#binarize\n",
    "for col in ['A1', 'A9', 'A10', 'A12', 'label']:\n",
    "    df[col] = to_binary(df, col)\n",
    "    \n",
    "onehot_cols = ['A4', 'A5', 'A6', 'A7', 'A13']\n",
    "\n",
    "#perform one hot encoding, and drop original columns\n",
    "df  = df.join(pd.get_dummies(df[onehot_cols], dtype=int))\\\n",
    "                                .drop(onehot_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A1         int64\n",
       "A2       float64\n",
       "A3       float64\n",
       "A8       float64\n",
       "A9         int64\n",
       "A10        int64\n",
       "A11      float64\n",
       "A12        int64\n",
       "A14      float64\n",
       "A15      float64\n",
       "label      int64\n",
       "A4_l       int64\n",
       "A4_u       int64\n",
       "A4_y       int64\n",
       "A5_g       int64\n",
       "A5_gg      int64\n",
       "A5_p       int64\n",
       "A6_aa      int64\n",
       "A6_c       int64\n",
       "A6_cc      int64\n",
       "A6_d       int64\n",
       "A6_e       int64\n",
       "A6_ff      int64\n",
       "A6_i       int64\n",
       "A6_j       int64\n",
       "A6_k       int64\n",
       "A6_m       int64\n",
       "A6_q       int64\n",
       "A6_r       int64\n",
       "A6_w       int64\n",
       "A6_x       int64\n",
       "A7_bb      int64\n",
       "A7_dd      int64\n",
       "A7_ff      int64\n",
       "A7_h       int64\n",
       "A7_j       int64\n",
       "A7_n       int64\n",
       "A7_o       int64\n",
       "A7_v       int64\n",
       "A7_z       int64\n",
       "A13_g      int64\n",
       "A13_p      int64\n",
       "A13_s      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "      <th>...</th>\n",
       "      <th>A7_ff</th>\n",
       "      <th>A7_h</th>\n",
       "      <th>A7_j</th>\n",
       "      <th>A7_n</th>\n",
       "      <th>A7_o</th>\n",
       "      <th>A7_v</th>\n",
       "      <th>A7_z</th>\n",
       "      <th>A13_g</th>\n",
       "      <th>A13_p</th>\n",
       "      <th>A13_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.46</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1     A2    A3    A8  A9  A10  A11  A12    A14    A15  ...  A7_ff  A7_h  \\\n",
       "0   0  30.83  0.00  1.25   0    0  1.0    0  202.0    0.0  ...      0     0   \n",
       "1   1  58.67  4.46  3.04   0    0  6.0    0   43.0  560.0  ...      0     1   \n",
       "\n",
       "   A7_j  A7_n  A7_o  A7_v  A7_z  A13_g  A13_p  A13_s  \n",
       "0     0     0     0     1     0      1      0      0  \n",
       "1     0     0     0     0     0      1      0      0  \n",
       "\n",
       "[2 rows x 43 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Real People's Data\n",
    "\n",
    "To illustrate how this model would work in real life, I want to simulate this data belonging to people. I am generating random names to be associated with each row. I know that this is not an ideal example since I am infact starting with the data all collated on my computer with peoples names and data being directly exposed. Not differentially private at all..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Darrin Ware'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import names #used to get random names\n",
    "names.get_first_name()+' ' +names.get_last_name() #call random name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "used_names = set()\n",
    "for idx in range(len(df)):\n",
    "    name = names.get_first_name()+' ' +names.get_last_name()\n",
    "    while name in used_names:\n",
    "        name = names.get_first_name()+' ' +names.get_last_name()\n",
    "        \n",
    "    used_names.add(name)\n",
    "    users.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "      <th>...</th>\n",
       "      <th>A7_h</th>\n",
       "      <th>A7_j</th>\n",
       "      <th>A7_n</th>\n",
       "      <th>A7_o</th>\n",
       "      <th>A7_v</th>\n",
       "      <th>A7_z</th>\n",
       "      <th>A13_g</th>\n",
       "      <th>A13_p</th>\n",
       "      <th>A13_s</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Michael Chalker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.46</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Robert Sandler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1     A2    A3    A8  A9  A10  A11  A12    A14    A15  ...  A7_h  A7_j  \\\n",
       "0   0  30.83  0.00  1.25   0    0  1.0    0  202.0    0.0  ...     0     0   \n",
       "1   1  58.67  4.46  3.04   0    0  6.0    0   43.0  560.0  ...     1     0   \n",
       "\n",
       "   A7_n  A7_o  A7_v  A7_z  A13_g  A13_p  A13_s             name  \n",
       "0     0     0     1     0      1      0      0  Michael Chalker  \n",
       "1     0     0     0     0      1      0      0   Robert Sandler  \n",
       "\n",
       "[2 rows x 44 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name'] = users\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get features and labels as numpy arrays which we can convert to tensors\n",
    "features = df.drop(['label', 'name'], axis=1).values.astype(float)\n",
    "labels = df['label'].values.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "I am using PyTorch to create a neural network to classify whether someone is accepted for credit or not. PyTorch integrates will with PySyft, the package used to encrypt our deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import syft as sy\n",
    "import torch as th\n",
    "\n",
    "data = th.tensor(features, dtype=th.float32, requires_grad=True)\n",
    "target = th.tensor(labels, dtype=th.int64, requires_grad=False).reshape(-1,1)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    '''\n",
    "    Neural Network Example Model\n",
    "    \n",
    "    Attributes\n",
    "    :hidden_layers (nn.ModuleList) - hidden units and dimensions for each layer of network\n",
    "    :output (nn.Linear) - final fully-connected layer that handles output for model\n",
    "    :dropout (nn.Dropout) - handling of layer-wise drop-out parameter\n",
    "    \n",
    "    Functions\n",
    "    :forward - handling of forward pass of datum through the network.\n",
    "    '''\n",
    "    def __init__(self, input_size, output_size, hidden_layers, drop_p=0.2):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
    "\n",
    "        #create hidden layers\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:]) #gives input/output sizes for each layer\n",
    "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "        self.dropout = nn.Dropout(p=drop_p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for each in self.hidden_layers:\n",
    "            x = F.relu(each(x)) #apply relu to each hidden node\n",
    "            x = self.dropout(x) #apply dropout\n",
    "        x = self.output(x) #apply output weights\n",
    "        return F.log_softmax(x, dim=-1) #apply activation log softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='classical_dl'></a>\n",
    "## Classical Deep Learning\n",
    "Here we train our network on data that is not distributed (therefore this is not yet a federated or encrypted problem). However, this exercise is useful in showing how we can transition from traditional deep learning to federated deep learning.\n",
    "\n",
    "First create a dataset of batch size one. This is realistic since most people would only have their own credit score data. This might be different if we decide to use a secure or trusted third party to manage parts of the data, but we don't trust the credit rating company with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [(data[i], target[i]) for i in range(len(data))]\n",
    "\n",
    "#instantiate model\n",
    "in_size = data[0].shape[0]\n",
    "out_size = 2\n",
    "hidden_layers=[25,10]\n",
    "model = Model(in_size, out_size, hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  0.0000,  30.8300,   0.0000,   1.2500,   0.0000,   0.0000,   1.0000,\n",
       "           0.0000, 202.0000,   0.0000,   0.0000,   1.0000,   0.0000,   1.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           1.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   1.0000,   0.0000,   1.0000,   0.0000,   0.0000],\n",
       "        grad_fn=<SelectBackward>), tensor([0]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data, _target = dataset[0]\n",
    "_data, _target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=42, out_features=25, bias=True)\n",
       "    (1): Linear(in_features=25, out_features=10, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=10, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = optim.SGD(params=model.parameters(), lr=0.1) #use a simple stochastic gradient descent optimizer\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new training logic to reflect federated learning\n",
    "def train(model, datasets, epochs, criterion, optimizer):\n",
    "    \n",
    "    steps=0\n",
    "    model.train() #training mode\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        running_loss=0\n",
    "        for ii, (data,target) in enumerate(datasets): #iterates over pointers to remote data\n",
    "            steps+=1\n",
    "            optimizer.zero_grad()#zero out gradients so that one forward pass doesnt pick up previous forward's gradients\n",
    "            outputs = model.forward(data) #make prediction\n",
    "            outputs = outputs.reshape(1,-1) #get shape of (1,2) as we need at least two dimension\n",
    "            loss = criterion(outputs,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #print(f\"step: {steps}\", loss.item())\n",
    "            running_loss+=loss.item()\n",
    "            #code below courtesy of udacity\n",
    "            \n",
    "            print_every=100\n",
    "            if steps % print_every == 0:\n",
    "                print('Train Epoch: {} [{}/{}]  \\tLoss: {:.6f}'.format(\n",
    "                    e, ii+1, len(datasets), loss.item()/print_every))\n",
    "                \n",
    "                running_loss=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [100/653]  \tLoss: 0.006315\n",
      "Train Epoch: 1 [200/653]  \tLoss: 0.231044\n",
      "Train Epoch: 1 [300/653]  \tLoss: 0.001275\n",
      "Train Epoch: 1 [400/653]  \tLoss: 0.002540\n",
      "Train Epoch: 1 [500/653]  \tLoss: 0.132581\n",
      "Train Epoch: 1 [600/653]  \tLoss: 0.002443\n"
     ]
    }
   ],
   "source": [
    "train(model, dataset, 1, nn.NLLLoss(), opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use PyTorch's `Dataset` class to make the processing of data a little easier, but for the purpose of this example it will not give any clear benefits. If you would like to read more about PyTorch's abstract `Dataset` class [read here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html), with another example [here](https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel). Generally speaking, using `Dataset` and `DataLoader` makes the handling of training and testing data much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "dataset_ = TensorDataset(data, target.view(-1))\n",
    "data_loader = DataLoader(dataset_, batch_size=1, shuffle=True) #this gives us an identical implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [100/653]  \tLoss: 0.003197\n",
      "Train Epoch: 1 [200/653]  \tLoss: 0.004868\n",
      "Train Epoch: 1 [300/653]  \tLoss: 0.023472\n",
      "Train Epoch: 1 [400/653]  \tLoss: 0.004868\n",
      "Train Epoch: 1 [500/653]  \tLoss: 0.005587\n",
      "Train Epoch: 1 [600/653]  \tLoss: 0.007802\n",
      "CPU times: user 702 ms, sys: 4.76 ms, total: 707 ms\n",
      "Wall time: 706 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#training loss will look a little different since the dataset is shuffled\n",
    "\n",
    "train(model, data_loader, 1, nn.NLLLoss(), opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a credit application model that is training on our data. However, this is by no means yet federated learning. The implementation above simply trains a model with a batch size of 1. We will federate the model in the upcoming section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Federated Learning Network\n",
    "The idea behind this is that we train a model on subsets of data (encrypted or otherwise) that never leaves the ownership of an individual. In this example of credit rating scores it would allow people to submit claims without ever losing ownership of their data. It requires very little trust of the party to which the application is being submitted.\n",
    "\n",
    "Even though we currently have our dataset located locally, we want to simulate having many people in our network who each maintain ownership of their data. Therefore we have to create a virtual worker for each datum. The work/data flow in this situation would be as follows:\n",
    "\n",
    "- get pointers to training data on each remote worker\n",
    "*Training Steps:*\n",
    "- send model to remote worker\n",
    "- train model on data located with remote worker\n",
    "- recieve updated model from remote worker\n",
    "- repeat for all workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#send data to remote workers\n",
    "data_bob = data_bob.send(bob)\n",
    "data_alice = data_alice.send(alice)\n",
    "\n",
    "target_bob = target_bob.send(bob)\n",
    "target_alice = target_alice.send(alice)\n",
    "\n",
    "datasets = [(data_bob, target_bob), (data_alice, target_alice)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(th)\n",
    "workers = [sy.VirtualWorker(hook, id=name) for name in df.name.str.replace(' ', '').values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Data to Remote Worker\n",
    "In reality the data of each person would already be on a remote worker. Either each person's device or aggregated into multiple remote workers by a secure third party.\n",
    "\n",
    "Here we have two options:\n",
    "1. send the data to each worker individually\n",
    "2. use PySyft's implemenation of PyTorch's `Dataset` and `DataLoader`\n",
    "\n",
    "I will use PySyft's `BaseDataset`, `FederatedDataset` and `FederatedDataLoader` since this simplifies dataprocessing for larger applications, even though it is not necessary for this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[PointerTensor | me:79412454298 -> MichaelChalker:94676651787]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 1\n",
    "remote_dataset = []\n",
    "for i in range(len(dataset)):\n",
    "    d, t = dataset[i]\n",
    "    \n",
    "    r_d = d.send(workers[i])\n",
    "    r_t = t.send(workers[i])\n",
    "    \n",
    "    remote_dataset.append((r_d, r_t))\n",
    "    \n",
    "r_d, r_t = remote_dataset[0]\n",
    "r_d #this is now a pointer to remote data rather than an actual tensor on our device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MichaelChalker', 'RobertSandler', 'GayeNanney', 'KennethMckibben', 'DavidYancy']\n"
     ]
    }
   ],
   "source": [
    "# Option 2\n",
    "# Cast the result in BaseDatasets\n",
    "remote_dataset_list = []\n",
    "for i in range(len(dataset)):\n",
    "    d, t = dataset[i] #get data\n",
    "\n",
    "    #send to worker before adding to dataset\n",
    "    r_d = d.reshape(1,-1).send(workers[i])\n",
    "    r_t = t.send(workers[i])\n",
    "    \n",
    "    dtset = sy.BaseDataset(r_d, r_t)\n",
    "    remote_dataset_list.append(dtset)\n",
    "\n",
    "# Build the FederatedDataset object\n",
    "remote_dataset = sy.FederatedDataset(remote_dataset_list)\n",
    "print(remote_dataset.workers[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = sy.FederatedDataLoader(remote_dataset, batch_size=1, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new training logic to reflect federated learning\n",
    "def federated_train(model, datasets, epochs, criterion, optimizer):\n",
    "    print(f'Federated Training on {len(datasets)} remote workers (dataowners)')\n",
    "    steps=0\n",
    "    model.train() #training mode\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        running_loss=0\n",
    "        for ii, (data,target) in enumerate(datasets): #iterates over pointers to remote data\n",
    "            steps+=1\n",
    "            \n",
    "            #FEDERATION STEP\n",
    "            model.send(data.location) #send model to remote worker\n",
    "            \n",
    "            #NB the steps below all happen remotely\n",
    "            optimizer.zero_grad()#zero out gradients so that one forward pass doesnt pick up previous forward's gradients\n",
    "            outputs = model.forward(data) #make prediction\n",
    "            outputs = outputs.reshape(1,-1) #get shape of (1,2) as we need at least two dimension\n",
    "            loss = criterion(outputs,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #FEDERATION STEP\n",
    "            model.get() #get model with new gradients back from remote worker\n",
    "            \n",
    "            #FEDERATION STEP\n",
    "            _loss = loss.get() #get loss from remote worker\n",
    "            running_loss+=_loss\n",
    "            \n",
    "            print_every=100\n",
    "            if steps % print_every == 0:\n",
    "                print('Train Epoch: {} [{}/{}]  \\tLoss: {:.6f}'.format(\n",
    "                    e, ii+1, len(datasets), _loss/print_every))\n",
    "                \n",
    "                running_loss=0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated Training on 653 remote workers (dataowners)\n",
      "Train Epoch: 1 [100/653]  \tLoss: 0.009534\n",
      "Train Epoch: 1 [200/653]  \tLoss: 0.088674\n",
      "Train Epoch: 1 [300/653]  \tLoss: 0.008809\n",
      "Train Epoch: 1 [400/653]  \tLoss: 0.003607\n",
      "Train Epoch: 1 [500/653]  \tLoss: 0.022696\n",
      "Train Epoch: 1 [600/653]  \tLoss: 0.006318\n",
      "CPU times: user 4.54 s, sys: 29.6 ms, total: 4.57 s\n",
      "Wall time: 4.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "federated_train(model, train_loader, 1, nn.NLLLoss(), opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Viola!_ Now we have a federated model where the data never leaves the ownership of a remote device. We can implement this in a way where each user's device is a worker, or where we have a smaller number of workers (data owners) which are all third parties trusted by the credit applicants to take care of their data.\n",
    "\n",
    "Nevertheless, this **data is not yet encrypted** and we could deduce things specific to the applicant just by getting or looking at the remote data.\n",
    "\n",
    "Notice how the federated model is about 6.5x slower than the non-federated model. This is simply one of the trade-offs that we have to be willing to make."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
