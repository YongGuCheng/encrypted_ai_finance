{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "# BEWARE, ignoreing warnings is not always a good idea\n",
    "# I am doing it for presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Private and Encrypted AI - Credit Approval Application\n",
    "\n",
    "This notebook is meant for my exploratory development of en encrypted federated deep learning approach.\n",
    "I will develop a final model in a separate folder.\n",
    "\n",
    "### Glossary\n",
    "1. [Data Preparation & Setup](#data_prep)\n",
    "2. [Classical Deep Learning](#classical_dl)\n",
    "3. [Federated Deep Learning](#federated_dl)<br>\n",
    "    3.1 [Model Averaging with Trusted Aggregator](#fl_model_avg)\n",
    "4. [Encrypted Deep Learning](#encrypted_dl)<br>\n",
    "   4.1 [Secured Multi-Party Computation (SMPC)](#smpc) <br>\n",
    "   4.2 [Encrypted Gradient Averaging](#fl_encrypt_avg)<br>\n",
    "   4.3 [Differential Privacy for DL](#dp_dl)\n",
    "   \n",
    "<hr>\n",
    "\n",
    "_Notes_ <br>This project was inspired by lectures of [Andrew Trask](https://iamtrask.github.io/) in the [Private AI Scholarship Challenge on Udacity](https://www.udacity.com/facebook-AI-scholarship). Furthermore, segments of the code are inspired by the [PySyft tutorials on GitHub](https://github.com/OpenMined/PySyft/tree/dev/examples/tutorials); an excellent resource for people starting off with Private AI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_prep'></a>\n",
    "## Data Preparation\n",
    "- only using non-NaN values. I drop NaN values because the dataset is not very big regardless, and we are not dropping very many values.\n",
    "- Convert binary variables to a numeric representation, and one-hot-encode categorical variables. We do not want to use label encoder since a label encoder would make it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [ f\"A{i}\" for i in range(1,16)]\n",
    "cols.append('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(653, 16) \n",
      " ------- \n",
      "\n",
      "  A1     A2    A3 A4 A5 A6 A7    A8 A9 A10  A11 A12 A13    A14  A15 label\n",
      "0  b  30.83  0.00  u  g  w  v  1.25  t   t    1   f   g  00202    0     +\n",
      "1  a  58.67  4.46  u  g  q  h  3.04  t   t    6   f   g  00043  560     +\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/crx.data', names=cols)\\\n",
    "    .replace(to_replace='?', value=np.nan).dropna()\n",
    "print(df.shape, \"\\n ------- \\n\")\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "\n",
    "Let's check out what this data looks like first, so that we have an idea of what we are dealing with. In true encrypted, federated learning we would not have this luxury though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def to_binary(df, col):\n",
    "    u = df[col].unique()\n",
    "    mapping =dict(zip(u, [i for i in range(0,len(u))]))\n",
    "    return df[col].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b\n",
       "1    a\n",
       "2    a\n",
       "3    b\n",
       "4    b\n",
       "Name: A1, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.A1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to float\n",
    "for col in ['A2', 'A3', 'A8', 'A11', 'A14', 'A15']:\n",
    "    df[col] = df[col].astype(float)\n",
    "    \n",
    "#binarize\n",
    "for col in ['A1', 'A9', 'A10', 'A12', 'label']:\n",
    "    df[col] = to_binary(df, col)\n",
    "    \n",
    "onehot_cols = ['A4', 'A5', 'A6', 'A7', 'A13']\n",
    "\n",
    "#perform one hot encoding, and drop original columns\n",
    "df  = df.join(pd.get_dummies(df[onehot_cols], dtype=int))\\\n",
    "                                .drop(onehot_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{dtype('int64'), dtype('float64')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.dtypes) #check that we have the data types we expect, no object types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A8</th>\n",
       "      <th>A11</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31.504</td>\n",
       "      <td>4.830</td>\n",
       "      <td>2.244</td>\n",
       "      <td>2.502</td>\n",
       "      <td>180.360</td>\n",
       "      <td>1013.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.838</td>\n",
       "      <td>5.027</td>\n",
       "      <td>3.371</td>\n",
       "      <td>4.968</td>\n",
       "      <td>168.297</td>\n",
       "      <td>5253.279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.580</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.000</td>\n",
       "      <td>73.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.420</td>\n",
       "      <td>2.835</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>160.000</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>38.250</td>\n",
       "      <td>7.500</td>\n",
       "      <td>2.625</td>\n",
       "      <td>3.000</td>\n",
       "      <td>272.000</td>\n",
       "      <td>400.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>76.750</td>\n",
       "      <td>28.000</td>\n",
       "      <td>28.500</td>\n",
       "      <td>67.000</td>\n",
       "      <td>2000.000</td>\n",
       "      <td>100000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A2      A3      A8     A11       A14         A15\n",
       "mean  31.504   4.830   2.244   2.502   180.360    1013.761\n",
       "std   11.838   5.027   3.371   4.968   168.297    5253.279\n",
       "min   13.750   0.000   0.000   0.000     0.000       0.000\n",
       "25%   22.580   1.040   0.165   0.000    73.000       0.000\n",
       "50%   28.420   2.835   1.000   0.000   160.000       5.000\n",
       "75%   38.250   7.500   2.625   3.000   272.000     400.000\n",
       "max   76.750  28.000  28.500  67.000  2000.000  100000.000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#distribution of numeric-only columns\n",
    "df[['A2', 'A3', 'A8', 'A11', 'A14', 'A15']].describe().iloc[1:, :10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "      <th>...</th>\n",
       "      <th>A7_ff</th>\n",
       "      <th>A7_h</th>\n",
       "      <th>A7_j</th>\n",
       "      <th>A7_n</th>\n",
       "      <th>A7_o</th>\n",
       "      <th>A7_v</th>\n",
       "      <th>A7_z</th>\n",
       "      <th>A13_g</th>\n",
       "      <th>A13_p</th>\n",
       "      <th>A13_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.46</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1     A2    A3    A8  A9  A10  A11  A12    A14    A15  ...  A7_ff  A7_h  \\\n",
       "0   0  30.83  0.00  1.25   0    0  1.0    0  202.0    0.0  ...      0     0   \n",
       "1   1  58.67  4.46  3.04   0    0  6.0    0   43.0  560.0  ...      0     1   \n",
       "\n",
       "   A7_j  A7_n  A7_o  A7_v  A7_z  A13_g  A13_p  A13_s  \n",
       "0     0     0     0     1     0      1      0      0  \n",
       "1     0     0     0     0     0      1      0      0  \n",
       "\n",
       "[2 rows x 43 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2) #double check what our DF looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Real People's Data\n",
    "\n",
    "To illustrate how this model would work in real life, I want to simulate this data belonging to people. I am generating random names to be associated with each row. I know that this is not an ideal example since I am in fact starting with the data all collated on my computer with peoples names and data being directly exposed. Not private at all..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jerry Canales'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import names #used to get random names\n",
    "names.get_first_name()+' ' +names.get_last_name() #call random name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "used_names = set()\n",
    "for idx in range(len(df)):\n",
    "    name = names.get_first_name()+' ' +names.get_last_name()\n",
    "    while name in used_names:\n",
    "        name = names.get_first_name()+' ' +names.get_last_name()\n",
    "        \n",
    "    used_names.add(name)\n",
    "    users.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "      <th>...</th>\n",
       "      <th>A7_h</th>\n",
       "      <th>A7_j</th>\n",
       "      <th>A7_n</th>\n",
       "      <th>A7_o</th>\n",
       "      <th>A7_v</th>\n",
       "      <th>A7_z</th>\n",
       "      <th>A13_g</th>\n",
       "      <th>A13_p</th>\n",
       "      <th>A13_s</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mandi Lerma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.46</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rebecca Caldwell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1     A2    A3    A8  A9  A10  A11  A12    A14    A15  ...  A7_h  A7_j  \\\n",
       "0   0  30.83  0.00  1.25   0    0  1.0    0  202.0    0.0  ...     0     0   \n",
       "1   1  58.67  4.46  3.04   0    0  6.0    0   43.0  560.0  ...     1     0   \n",
       "\n",
       "   A7_n  A7_o  A7_v  A7_z  A13_g  A13_p  A13_s              name  \n",
       "0     0     0     1     0      1      0      0       Mandi Lerma  \n",
       "1     0     0     0     0      1      0      0  Rebecca Caldwell  \n",
       "\n",
       "[2 rows x 44 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name'] = users\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get features and labels as numpy arrays which we can convert to tensors\n",
    "features = df.drop(['label', 'name'], axis=1).values.astype(float)\n",
    "labels = df['label'].values.astype(float)\n",
    "\n",
    "\n",
    "#normalize\n",
    "sclr = MinMaxScaler()\n",
    "features = sclr.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save features and labels for future use\n",
    "np.save('data/features', features)\n",
    "np.save('data/labels', labels)\n",
    "\n",
    "#save labels where shape is (1,2)\n",
    "labels=pd.get_dummies(df['label']).values.astype(float)\n",
    "np.save('data/labels_dim', labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Please Note_ <br>\n",
    "Normalization is not necessary per se for any machine learning algorithm, but it is recommended for deep learning for training purposes. Read more [here](https://datascience.stackexchange.com/a/13221/60648).\n",
    "\n",
    "## Model Development\n",
    "I am using PyTorch to create a neural network to classify whether someone is accepted for credit or not. PyTorch integrates will with PySyft, the package used to encrypt our deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkucz/p_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mkucz/p_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mkucz/p_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mkucz/p_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mkucz/p_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mkucz/p_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/mkucz/p_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mkucz/p_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mkucz/p_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mkucz/p_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mkucz/p_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mkucz/p_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0827 20:22:10.246440 139828562499392 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/home/mkucz/p_venv/lib/python3.6/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0827 20:22:10.255267 139828562499392 deprecation_wrapper.py:119] From /home/mkucz/p_venv/lib/python3.6/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import syft as sy\n",
    "import torch as th\n",
    "th.manual_seed(42) #so that dropout affects same layers\n",
    "\n",
    "data = th.tensor(features, dtype=th.float32, requires_grad=True)\n",
    "target = th.tensor(labels, dtype=th.float32, requires_grad=False).reshape(-1,2)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    '''\n",
    "    Neural Network Example Model\n",
    "    \n",
    "    Attributes\n",
    "    :hidden_layers (nn.ModuleList) - hidden units and dimensions for each layer\n",
    "    :output (nn.Linear) - final fully-connected layer to handle output for model\n",
    "    :dropout (nn.Dropout) - handling of layer-wise drop-out parameter\n",
    "    \n",
    "    Functions\n",
    "    :forward - handling of forward pass of datum through the network.\n",
    "    '''\n",
    "    def __init__(self, args):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(args.in_size,\n",
    "                                                      args.hidden_layers[0])])\n",
    "\n",
    "        #create hidden layers\n",
    "        layer_sizes = zip(args.hidden_layers[:-1], args.hidden_layers[1:]) \n",
    "        #gives input/output sizes for each layer\n",
    "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "        self.output = nn.Linear(args.hidden_layers[-1], args.out_size)\n",
    "        self.dropout = None if args.drop_p is None \\\n",
    "                                            else nn.Dropout(p=args.drop_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, args.in_size)\n",
    "        for each in self.hidden_layers:\n",
    "            x = F.relu(each(x)) #apply relu to each hidden node\n",
    "            \n",
    "            if self.dropout is not None:\n",
    "                x = self.dropout(x) #apply dropout\n",
    "                \n",
    "        x = self.output(x) #apply output weights\n",
    "        \n",
    "        if args.activation is None:\n",
    "            return x\n",
    "        \n",
    "        return args.activation(x, dim=args.dim) #apply activation log softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='classical_dl'></a>\n",
    "## Classical Deep Learning\n",
    "Here we train our network on data that is not distributed (therefore this is not yet a federated or encrypted problem). However, this exercise is useful in showing how we can transition from traditional deep learning to federated deep learning.\n",
    "\n",
    "First create a dataset of batch size one. This is realistic since most people would only have their own credit score data. This might be different if we decide to use a secure or trusted third party to manage parts of the data, but we don't trust the credit rating company with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self, in_size, out_size, hidden_layers,\n",
    "                       activation=F.softmax, dim=-1):\n",
    "        self.batch_size = 1\n",
    "        self.drop_p = None\n",
    "        self.epochs = 300\n",
    "        self.lr = 0.001\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.precision_fractional=10\n",
    "        self.activation = activation\n",
    "        self.dim = dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [(data[i], target[i].reshape(1,2)) for i in range(len(data))]\n",
    "\n",
    "#instantiate model\n",
    "in_size = data[0].shape[0]\n",
    "out_size = 2\n",
    "hidden_layers=[32,15,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000, 0.2711, 0.0000, 0.0439, 0.0000, 0.0000, 0.0149, 0.0000, 0.1010,\n",
       "         0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
       "        grad_fn=<SelectBackward>), tensor([[1., 0.]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data, _target = dataset[0]\n",
    "_data, _target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train(model, datasets, criterion):\n",
    "    #use a simple stochastic gradient descent optimizer\n",
    "    #define optimizer for each model\n",
    "    optimizer = optim.SGD(params=model.parameters(), lr=args.lr)\n",
    "    steps=0\n",
    "    model.train() #training mode\n",
    "    for e in range(1, args.epochs+1):\n",
    "        running_loss=0\n",
    "        for ii, (data,target) in enumerate(datasets): #iterates over pointers to remote data\n",
    "            steps+=1\n",
    "            optimizer.zero_grad()#zero out gradients so that one forward pass doesnt pick up previous forward's gradients\n",
    "            outputs = model.forward(data) #make prediction\n",
    "            outputs = outputs.reshape(1,-1) #get shape of (1,2) as we need at least two dimension\n",
    "            loss = criterion(outputs, target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #print(f\"step: {steps}\", loss.item())\n",
    "            running_loss+=loss.item()\n",
    "\n",
    "        if e%10==0:\n",
    "            print(f'Epoch: {e} \\tLoss: {running_loss/len(datasets):.6f}')\n",
    "        running_loss=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments(in_size, out_size, hidden_layers, activation=F.softmax, dim=1)\n",
    "base_model = Model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 0.247287\n",
      "Epoch: 2 \tLoss: 0.246489\n",
      "Epoch: 3 \tLoss: 0.245740\n",
      "Epoch: 4 \tLoss: 0.244993\n",
      "Epoch: 5 \tLoss: 0.244216\n",
      "Epoch: 6 \tLoss: 0.243396\n",
      "Epoch: 7 \tLoss: 0.242531\n",
      "Epoch: 8 \tLoss: 0.241591\n",
      "Epoch: 9 \tLoss: 0.240556\n",
      "Epoch: 10 \tLoss: 0.239403\n",
      "Epoch: 11 \tLoss: 0.238106\n",
      "Epoch: 12 \tLoss: 0.236645\n",
      "Epoch: 13 \tLoss: 0.234978\n",
      "Epoch: 14 \tLoss: 0.233058\n",
      "Epoch: 15 \tLoss: 0.230859\n",
      "Epoch: 16 \tLoss: 0.228302\n",
      "Epoch: 17 \tLoss: 0.225384\n",
      "Epoch: 18 \tLoss: 0.222019\n",
      "Epoch: 19 \tLoss: 0.218137\n",
      "Epoch: 20 \tLoss: 0.213731\n",
      "Epoch: 21 \tLoss: 0.208719\n",
      "Epoch: 22 \tLoss: 0.203057\n",
      "Epoch: 23 \tLoss: 0.196793\n",
      "Epoch: 24 \tLoss: 0.189950\n",
      "Epoch: 25 \tLoss: 0.182593\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-bfe29c64b50a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#exact replica of base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-118-7b1a026a984b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, datasets, criterion)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = copy.deepcopy(base_model) #exact replica of base model\n",
    "train(model, dataset, nn.MSELoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use PyTorch's `Dataset` class to make the processing of data a little easier, but for the purpose of this example it will not give any clear benefits. If you would like to read more about PyTorch's abstract `Dataset` class [read here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html), with another example [here](https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel). Generally speaking, using `Dataset` and `DataLoader` makes the handling of training and testing data much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "n_train_items = int(len(dataset)*0.7)\n",
    "n_test_items = len(dataset) - n_train_items\n",
    "\n",
    "train_dataset = TensorDataset(data[:n_train_items], target[:n_train_items])\n",
    "test_dataset = TensorDataset(data[:n_train_items], target[:n_train_items])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "#this gives us an identical implementation, but split up into train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \tLoss: 0.245377\n",
      "Epoch: 20 \tLoss: 0.239523\n",
      "Epoch: 30 \tLoss: 0.224637\n",
      "Epoch: 40 \tLoss: 0.181348\n",
      "Epoch: 50 \tLoss: 0.120974\n",
      "Epoch: 60 \tLoss: 0.099086\n",
      "Epoch: 70 \tLoss: 0.093512\n",
      "Epoch: 80 \tLoss: 0.091027\n",
      "Epoch: 90 \tLoss: 0.089384\n",
      "Epoch: 100 \tLoss: 0.088098\n",
      "Epoch: 110 \tLoss: 0.087058\n",
      "Epoch: 120 \tLoss: 0.086190\n",
      "Epoch: 130 \tLoss: 0.085509\n",
      "Epoch: 140 \tLoss: 0.084796\n",
      "Epoch: 150 \tLoss: 0.084161\n",
      "Epoch: 160 \tLoss: 0.083559\n",
      "Epoch: 170 \tLoss: 0.082943\n",
      "Epoch: 180 \tLoss: 0.082355\n",
      "Epoch: 190 \tLoss: 0.081759\n",
      "Epoch: 200 \tLoss: 0.081160\n",
      "Epoch: 210 \tLoss: 0.080565\n",
      "Epoch: 220 \tLoss: 0.079908\n",
      "Epoch: 230 \tLoss: 0.079232\n",
      "Epoch: 240 \tLoss: 0.078528\n",
      "Epoch: 250 \tLoss: 0.077749\n",
      "Epoch: 260 \tLoss: 0.076891\n",
      "Epoch: 270 \tLoss: 0.076085\n",
      "Epoch: 280 \tLoss: 0.075072\n",
      "Epoch: 290 \tLoss: 0.074087\n",
      "Epoch: 300 \tLoss: 0.073109\n",
      "CPU times: user 1min 8s, sys: 2.1 s, total: 1min 10s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#training loss will look a little different since the dataset is shuffled\n",
    "model = copy.deepcopy(base_model)\n",
    "train(model, train_loader, nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, criterion):\n",
    "    print(f'Testing')\n",
    "    steps = 0\n",
    "    model.eval()  # training mode\n",
    "\n",
    "    pred = []\n",
    "    true = []\n",
    "    running_loss=0.\n",
    "    for ii, (data, target) in tqdm_notebook(enumerate(dataloader),\n",
    "                                            unit='datum', desc='testing',\n",
    "                                            total=len(dataloader)):\n",
    "        # iterates over pointers to remote data\n",
    "        steps += 1\n",
    "        outputs = model.forward(data) #make prediction\n",
    "        outputs = outputs.reshape(1,-1)\n",
    "        #get shape of (1,2) as we need at least two dimension\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        _, y_pred = th.max(outputs, 1)\n",
    "        _, y_true = th.max(target, 1)\n",
    "\n",
    "        pred.append(y_pred.item())\n",
    "        true.append(y_true.item())\n",
    "\n",
    "        # get loss from remote worker and unencrypt\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print('Testing Loss: {:.6f}'.format(running_loss/len(dataset)))\n",
    "    \n",
    "    return pred, true\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a228f0f2b42340468f6a933f2b87f11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='testing', max=457, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Loss: 0.053298\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_true = test(model, test_loader, nn.MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[197  27]\n",
      " [ 15 218]]\n",
      "accuracy: 0.91\n",
      "recall: 0.89\n",
      "precision: 0.94\n"
     ]
    }
   ],
   "source": [
    "cnf_mtx = confusion_matrix(y_pred, y_true).astype(int)\n",
    "print(cnf_mtx)\n",
    "\n",
    "tp = cnf_mtx[1][1]\n",
    "tn = cnf_mtx[0][0]\n",
    "total = cnf_mtx.sum()\n",
    "\n",
    "print(f\"accuracy: {(tp+tn)/total:.2f}\")\n",
    "print(f\"recall: {tp/sum(y_true):.2f}\")\n",
    "print(f\"precision: {tp/sum(y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a credit application model that is training on our data. However, this is by no means yet federated learning. The implementation above simply trains a model with a batch size of 1. We will federate the model in the upcoming section [here]().\n",
    "\n",
    "Generally, the model looks pretty solid across the test sets. I will save the model parameters here so that we can use them in our federated or encrypted models (so as not to train it all from scratch again).\n",
    "\n",
    "Also, it took 70 seconds to train this model which amounts to about 0.233 seconds per epoch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.183px",
    "left": "910px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
