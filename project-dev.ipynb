{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# BEWARE, ignoreing warnings is not always a good idea\n",
    "# I am doing it for presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Private and Encrypted AI - Credit Approval Application\n",
    "\n",
    "This notebook is meant for my exploratory development of en encrypted federated deep learning approach.\n",
    "I will develop a final model in a separate folder.\n",
    "\n",
    "### Glossary\n",
    "1. [Data Preparation & Setup](#data_prep)\n",
    "2. [Classical Deep Learning](#classical_dl)\n",
    "3. [Federated Deep Learning](#federated_dl)<br>\n",
    "    3.1 [Model Averaging with Trusted Aggregator](#fl_model_avg)\n",
    "4. [Encrypted Deep Learning](#encrypted_dl)<br>\n",
    "   4.1 [Secured Multi-Party Computation (SMPC)](#smpc) <br>\n",
    "   4.2 [Encrypted Gradient Averaging](#fl_encrypt_avg)<br>\n",
    "   4.3 [Differential Privacy for DL](#dp_dl)\n",
    "   \n",
    "<hr>\n",
    "\n",
    "_Notes_ <br>This project was inspired by lectures of [Andrew Trask](https://iamtrask.github.io/) in the [Private AI Scholarship Challenge on Udacity](https://www.udacity.com/facebook-AI-scholarship). Furthermore, segments of the code are inspired by the [PySyft tutorials on GitHub](https://github.com/OpenMined/PySyft/tree/dev/examples/tutorials); an excellent resource for people starting off with Private AI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_prep'></a>\n",
    "## Data Preparation\n",
    "- only using non-NaN values. I drop NaN values because the dataset is not very big regardless, and we are not dropping very many values.\n",
    "- Convert binary variables to a numeric representation, and one-hot-encode categorical variables. We do not want to use label encoder since a label encoder would make it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [ f\"A{i}\" for i in range(1,16)]\n",
    "cols.append('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(653, 16) \n",
      " ------- \n",
      "\n",
      "  A1     A2    A3 A4 A5 A6 A7    A8 A9 A10  A11 A12 A13    A14  A15 label\n",
      "0  b  30.83  0.00  u  g  w  v  1.25  t   t    1   f   g  00202    0     +\n",
      "1  a  58.67  4.46  u  g  q  h  3.04  t   t    6   f   g  00043  560     +\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/crx.data', names=cols)\\\n",
    "    .replace(to_replace='?', value=np.nan).dropna()\n",
    "print(df.shape, \"\\n ------- \\n\")\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "\n",
    "Let's check out what this data looks like first, so that we have an idea of what we are dealing with. In true encrypted, federated learning we would not have this luxury though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def to_binary(df, col):\n",
    "    u = df[col].unique()\n",
    "    mapping =dict(zip(u, [i for i in range(0,len(u))]))\n",
    "    return df[col].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b\n",
       "1    a\n",
       "2    a\n",
       "3    b\n",
       "4    b\n",
       "Name: A1, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.A1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to float\n",
    "for col in ['A2', 'A3', 'A8', 'A11', 'A14', 'A15']:\n",
    "    df[col] = df[col].astype(float)\n",
    "    \n",
    "#binarize\n",
    "for col in ['A1', 'A9', 'A10', 'A12', 'label']:\n",
    "    df[col] = to_binary(df, col)\n",
    "    \n",
    "onehot_cols = ['A4', 'A5', 'A6', 'A7', 'A13']\n",
    "\n",
    "#perform one hot encoding, and drop original columns\n",
    "df  = df.join(pd.get_dummies(df[onehot_cols], dtype=int))\\\n",
    "                                .drop(onehot_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{dtype('int64'), dtype('float64')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.dtypes) #check that we have the data types we expect, no object types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A8</th>\n",
       "      <th>A11</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31.504</td>\n",
       "      <td>4.830</td>\n",
       "      <td>2.244</td>\n",
       "      <td>2.502</td>\n",
       "      <td>180.360</td>\n",
       "      <td>1013.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.838</td>\n",
       "      <td>5.027</td>\n",
       "      <td>3.371</td>\n",
       "      <td>4.968</td>\n",
       "      <td>168.297</td>\n",
       "      <td>5253.279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.580</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.000</td>\n",
       "      <td>73.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.420</td>\n",
       "      <td>2.835</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>160.000</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>38.250</td>\n",
       "      <td>7.500</td>\n",
       "      <td>2.625</td>\n",
       "      <td>3.000</td>\n",
       "      <td>272.000</td>\n",
       "      <td>400.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>76.750</td>\n",
       "      <td>28.000</td>\n",
       "      <td>28.500</td>\n",
       "      <td>67.000</td>\n",
       "      <td>2000.000</td>\n",
       "      <td>100000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A2      A3      A8     A11       A14         A15\n",
       "mean  31.504   4.830   2.244   2.502   180.360    1013.761\n",
       "std   11.838   5.027   3.371   4.968   168.297    5253.279\n",
       "min   13.750   0.000   0.000   0.000     0.000       0.000\n",
       "25%   22.580   1.040   0.165   0.000    73.000       0.000\n",
       "50%   28.420   2.835   1.000   0.000   160.000       5.000\n",
       "75%   38.250   7.500   2.625   3.000   272.000     400.000\n",
       "max   76.750  28.000  28.500  67.000  2000.000  100000.000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#distribution of numeric-only columns\n",
    "df[['A2', 'A3', 'A8', 'A11', 'A14', 'A15']].describe().iloc[1:, :10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "      <th>...</th>\n",
       "      <th>A7_ff</th>\n",
       "      <th>A7_h</th>\n",
       "      <th>A7_j</th>\n",
       "      <th>A7_n</th>\n",
       "      <th>A7_o</th>\n",
       "      <th>A7_v</th>\n",
       "      <th>A7_z</th>\n",
       "      <th>A13_g</th>\n",
       "      <th>A13_p</th>\n",
       "      <th>A13_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.46</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1     A2    A3    A8  A9  A10  A11  A12    A14    A15  ...  A7_ff  A7_h  \\\n",
       "0   0  30.83  0.00  1.25   0    0  1.0    0  202.0    0.0  ...      0     0   \n",
       "1   1  58.67  4.46  3.04   0    0  6.0    0   43.0  560.0  ...      0     1   \n",
       "\n",
       "   A7_j  A7_n  A7_o  A7_v  A7_z  A13_g  A13_p  A13_s  \n",
       "0     0     0     0     1     0      1      0      0  \n",
       "1     0     0     0     0     0      1      0      0  \n",
       "\n",
       "[2 rows x 43 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2) #double check what our DF looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Real People's Data\n",
    "\n",
    "To illustrate how this model would work in real life, I want to simulate this data belonging to people. I am generating random names to be associated with each row. I know that this is not an ideal example since I am in fact starting with the data all collated on my computer with peoples names and data being directly exposed. Not private at all..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Maria Crowe'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import names #used to get random names\n",
    "names.get_first_name()+' ' +names.get_last_name() #call random name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "used_names = set()\n",
    "for idx in range(len(df)):\n",
    "    name = names.get_first_name()+' ' +names.get_last_name()\n",
    "    while name in used_names:\n",
    "        name = names.get_first_name()+' ' +names.get_last_name()\n",
    "        \n",
    "    used_names.add(name)\n",
    "    users.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "      <th>...</th>\n",
       "      <th>A7_h</th>\n",
       "      <th>A7_j</th>\n",
       "      <th>A7_n</th>\n",
       "      <th>A7_o</th>\n",
       "      <th>A7_v</th>\n",
       "      <th>A7_z</th>\n",
       "      <th>A13_g</th>\n",
       "      <th>A13_p</th>\n",
       "      <th>A13_s</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rufus Owston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.46</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>David Johnson</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1     A2    A3    A8  A9  A10  A11  A12    A14    A15  ...  A7_h  A7_j  \\\n",
       "0   0  30.83  0.00  1.25   0    0  1.0    0  202.0    0.0  ...     0     0   \n",
       "1   1  58.67  4.46  3.04   0    0  6.0    0   43.0  560.0  ...     1     0   \n",
       "\n",
       "   A7_n  A7_o  A7_v  A7_z  A13_g  A13_p  A13_s           name  \n",
       "0     0     0     1     0      1      0      0   Rufus Owston  \n",
       "1     0     0     0     0      1      0      0  David Johnson  \n",
       "\n",
       "[2 rows x 44 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name'] = users\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get features and labels as numpy arrays which we can convert to tensors\n",
    "features = df.drop(['label', 'name'], axis=1).values.astype(float)\n",
    "labels = df['label'].values.astype(float)\n",
    "#labels=pd.get_dummies(df['label']).values.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Please Note_ <br>\n",
    "Normalization is not necessary for any machine learning algorithm, but it is recommended for deep learning for training purposes. Read more [here](https://datascience.stackexchange.com/a/13221/60648).\n",
    "\n",
    "## Model Development\n",
    "I am using PyTorch to create a neural network to classify whether someone is accepted for credit or not. PyTorch integrates will with PySyft, the package used to encrypt our deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     10
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkucz/p_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/mkucz/p_venv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:5506: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/mkucz/.keras/keras.json' mode='r' encoding='UTF-8'>\n",
      "  _config = json.load(open(_config_path))\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/mkucz/p_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0819 21:28:49.545506 140361539536704 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/home/mkucz/p_venv/lib/python3.6/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0819 21:28:49.553843 140361539536704 deprecation_wrapper.py:119] From /home/mkucz/p_venv/lib/python3.6/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-97d4b43839e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#so that dropout affects same layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import syft as sy\n",
    "import torch as th\n",
    "th.manual_seed(42) #so that dropout affects same layers\n",
    "\n",
    "data = th.tensor(features, dtype=th.float32, requires_grad=True)\n",
    "target = th.tensor(labels, dtype=th.int64, requires_grad=False).reshape(-1,1)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    '''\n",
    "    Neural Network Example Model\n",
    "    \n",
    "    Attributes\n",
    "    :hidden_layers (nn.ModuleList) - hidden units and dimensions for each layer of network\n",
    "    :output (nn.Linear) - final fully-connected layer that handles output for model\n",
    "    :dropout (nn.Dropout) - handling of layer-wise drop-out parameter\n",
    "    \n",
    "    Functions\n",
    "    :forward - handling of forward pass of datum through the network.\n",
    "    '''\n",
    "    def __init__(self, args):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(args.in_size, args.hidden_layers[0])])\n",
    "\n",
    "        #create hidden layers\n",
    "        layer_sizes = zip(args.hidden_layers[:-1], args.hidden_layers[1:]) #gives input/output sizes for each layer\n",
    "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "        self.output = nn.Linear(args.hidden_layers[-1], args.out_size)\n",
    "        self.dropout = None if args.drop_p is not None else nn.Dropout(p=args.drop_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for each in self.hidden_layers:\n",
    "            x = F.relu(each(x)) #apply relu to each hidden node\n",
    "            \n",
    "            if self.dropout is not None:\n",
    "                x = self.dropout(x) #apply dropout\n",
    "                \n",
    "        x = self.output(x) #apply output weights\n",
    "        return args.activation(x, dim=args.dim) #apply activation log softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='classical_dl'></a>\n",
    "## Classical Deep Learning\n",
    "Here we train our network on data that is not distributed (therefore this is not yet a federated or encrypted problem). However, this exercise is useful in showing how we can transition from traditional deep learning to federated deep learning.\n",
    "\n",
    "First create a dataset of batch size one. This is realistic since most people would only have their own credit score data. This might be different if we decide to use a secure or trusted third party to manage parts of the data, but we don't trust the credit rating company with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self, in_size, out_size, hidden_layers, activation=F.log_softmax, dim=-1):\n",
    "        self.batch_size = 1\n",
    "        self.drop_p = 0.2\n",
    "        self.epochs = 5\n",
    "        self.lr = 0.001\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.precision_fractional=10\n",
    "        self.activation = activation\n",
    "        self.dim = dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [(data[i], target[i]) for i in range(len(data))]\n",
    "\n",
    "#instantiate model\n",
    "in_size = data[0].shape[0]\n",
    "out_size = 2\n",
    "hidden_layers=[30,20,10,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments(in_size, out_size, hidden_layers)\n",
    "base_model = Model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  0.0000,  30.8300,   0.0000,   1.2500,   0.0000,   0.0000,   1.0000,\n",
       "           0.0000, 202.0000,   0.0000,   0.0000,   1.0000,   0.0000,   1.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           1.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   1.0000,   0.0000,   1.0000,   0.0000,   0.0000],\n",
       "        grad_fn=<SelectBackward>), tensor([0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data, _target = dataset[0]\n",
    "_data, _target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train(model, datasets, criterion):\n",
    "    #use a simple stochastic gradient descent optimizer\n",
    "    #define optimizer for each model\n",
    "    optimizer = optim.SGD(params=model.parameters(), lr=args.lr)\n",
    "    steps=0\n",
    "    model.train() #training mode\n",
    "    for e in range(1, args.epochs+1):\n",
    "        running_loss=0\n",
    "        for ii, (data,target) in enumerate(datasets): #iterates over pointers to remote data\n",
    "            steps+=1\n",
    "            optimizer.zero_grad()#zero out gradients so that one forward pass doesnt pick up previous forward's gradients\n",
    "            outputs = model.forward(data) #make prediction\n",
    "            outputs = outputs.reshape(1,-1) #get shape of (1,2) as we need at least two dimension\n",
    "            loss = criterion(outputs, target)\n",
    "            #loss = ((outputs - target.float())**2).sum()\n",
    "            '''\n",
    "            print(outputs, target, outputs-target.float())\n",
    "            print(outputs.shape[0])\n",
    "            print(loss)\n",
    "            break\n",
    "            '''\n",
    "            \n",
    "            #loss = criterion(outputs,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #print(f\"step: {steps}\", loss.item())\n",
    "            running_loss+=loss.item()\n",
    "\n",
    "            print_every = 200\n",
    "            if (ii+1)%print_every==0:\n",
    "                print(f'Epoch: {e} [{ii+1}/{len(datasets)}] \\tLoss: {running_loss/print_every:.6f}')\n",
    "                running_loss=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [200/653] \tLoss: 0.335800\n",
      "Epoch: 1 [400/653] \tLoss: 4.242975\n",
      "Epoch: 1 [600/653] \tLoss: 0.680553\n",
      "Epoch: 2 [200/653] \tLoss: 0.779483\n",
      "Epoch: 2 [400/653] \tLoss: 0.646549\n",
      "Epoch: 2 [600/653] \tLoss: 0.680794\n",
      "Epoch: 3 [200/653] \tLoss: 0.772166\n",
      "Epoch: 3 [400/653] \tLoss: 0.649821\n",
      "Epoch: 3 [600/653] \tLoss: 0.681052\n",
      "Epoch: 4 [200/653] \tLoss: 0.766927\n",
      "Epoch: 4 [400/653] \tLoss: 0.652247\n",
      "Epoch: 4 [600/653] \tLoss: 0.681284\n",
      "Epoch: 5 [200/653] \tLoss: 0.763166\n",
      "Epoch: 5 [400/653] \tLoss: 0.654033\n",
      "Epoch: 5 [600/653] \tLoss: 0.681475\n"
     ]
    }
   ],
   "source": [
    "model = base_model.copy()\n",
    "train(model, dataset, nn.NLLLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use PyTorch's `Dataset` class to make the processing of data a little easier, but for the purpose of this example it will not give any clear benefits. If you would like to read more about PyTorch's abstract `Dataset` class [read here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html), with another example [here](https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel). Generally speaking, using `Dataset` and `DataLoader` makes the handling of training and testing data much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "dataset_ = TensorDataset(data, target.view(-1))\n",
    "data_loader = DataLoader(dataset_, batch_size=1, shuffle=False) #this gives us an identical implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [200/653] \tLoss: 0.335800\n",
      "Epoch: 1 [400/653] \tLoss: 4.242975\n",
      "Epoch: 1 [600/653] \tLoss: 0.680553\n",
      "Epoch: 2 [200/653] \tLoss: 0.779483\n",
      "Epoch: 2 [400/653] \tLoss: 0.646549\n",
      "Epoch: 2 [600/653] \tLoss: 0.680794\n",
      "Epoch: 3 [200/653] \tLoss: 0.772166\n",
      "Epoch: 3 [400/653] \tLoss: 0.649821\n",
      "Epoch: 3 [600/653] \tLoss: 0.681052\n",
      "Epoch: 4 [200/653] \tLoss: 0.766927\n",
      "Epoch: 4 [400/653] \tLoss: 0.652247\n",
      "Epoch: 4 [600/653] \tLoss: 0.681284\n",
      "Epoch: 5 [200/653] \tLoss: 0.763166\n",
      "Epoch: 5 [400/653] \tLoss: 0.654033\n",
      "Epoch: 5 [600/653] \tLoss: 0.681475\n",
      "CPU times: user 4.3 s, sys: 69.6 ms, total: 4.37 s\n",
      "Wall time: 4.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#training loss will look a little different since the dataset is shuffled\n",
    "model = base_model.copy()\n",
    "train(model, data_loader, nn.NLLLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a credit application model that is training on our data. However, this is by no means yet federated learning. The implementation above simply trains a model with a batch size of 1. We will federate the model in the upcoming section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"federated_dl\"></a>\n",
    "## Federated Deep Learning\n",
    "The idea behind federated learning is that we train a model on subsets of data (encrypted or otherwise) that never leaves the ownership of an individual. In this example of credit rating scores it would allow people to submit claims without ever losing ownership of their data. It requires very little trust of the party to which the application is being submitted.\n",
    "\n",
    "Even though we currently have our dataset located locally, we want to simulate having many people in our network who each maintain ownership of their data. Therefore we have to create a virtual worker for each datum. The work/data flow in this situation would be as follows:\n",
    "\n",
    "- get pointers to training data on each remote worker <br>\n",
    "**Training Steps:**\n",
    "- send model to remote worker\n",
    "- train model on data located with remote worker\n",
    "- receive updated model from remote worker\n",
    "- repeat for all workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_workers(n_workers):\n",
    "    return [sy.VirtualWorker(hook, id=name) for name in df.name.str.replace(' ', '').values[:n_workers]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 22:21:27.369957 140383773411136 hook.py:98] Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(th)\n",
    "workers = connect_to_workers(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<VirtualWorker id:PaulinePike #objects:10>,\n",
       " <VirtualWorker id:SharonReed #objects:12>,\n",
       " <VirtualWorker id:AlyshaWalker #objects:12>,\n",
       " <VirtualWorker id:StevenSpeziale #objects:12>,\n",
       " <VirtualWorker id:LouisLembke #objects:12>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Data to Remote Worker\n",
    "In reality the data of each person would already be on a remote worker. Either each person's device or aggregated into multiple remote workers by a secure third party.\n",
    "\n",
    "Here we have two options:\n",
    "1. send the data to each worker individually\n",
    "2. use PySyft's implementation of PyTorch's `Dataset` and `DataLoader`\n",
    "\n",
    "I will use PySyft's `BaseDataset`, `FederatedDataset` and `FederatedDataLoader` since this simplifies dataprocessing for larger applications, even though it is not necessary for this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[PointerTensor | me:89706398710 -> PaulinePike:18834545422]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 1\n",
    "remote_dataset = []\n",
    "for i in range(len(dataset)):\n",
    "    d, t = dataset[i]\n",
    "    \n",
    "    r_d = d.send(workers[i])\n",
    "    r_t = t.send(workers[i])\n",
    "    \n",
    "    remote_dataset.append((r_d, r_t))\n",
    "    \n",
    "r_d, r_t = remote_dataset[0]\n",
    "r_d #this is now a pointer to remote data rather than an actual tensor on our device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MichaelBerry', 'SaraHoyos', 'KevinMack', 'DominickKern', 'SandraSmith']\n"
     ]
    }
   ],
   "source": [
    "# Option 2\n",
    "# Cast the result in BaseDatasets\n",
    "remote_dataset_list = []\n",
    "for i in range(len(dataset)):\n",
    "    d, t = dataset[i] #get data\n",
    "\n",
    "    #send to worker before adding to dataset\n",
    "    r_d = d.reshape(1,-1).send(workers[i])\n",
    "    r_t = t.send(workers[i])\n",
    "    \n",
    "    dtset = sy.BaseDataset(r_d, r_t)\n",
    "    remote_dataset_list.append(dtset)\n",
    "\n",
    "# Build the FederatedDataset object\n",
    "remote_dataset = sy.FederatedDataset(remote_dataset_list)\n",
    "print(remote_dataset.workers[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = sy.FederatedDataLoader(remote_dataset, batch_size=1,\n",
    "                                      shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#new training logic to reflect federated learning\n",
    "def federated_train(model, datasets, criterion):\n",
    "    #use a simple stochastic gradient descent optimizer\n",
    "    #define optimizer for each model\n",
    "    optimizer = optim.SGD(params=model.parameters(), lr=args.lr)\n",
    "    \n",
    "    print(f'Federated Training on {len(datasets)} remote workers (dataowners)')\n",
    "    steps=0\n",
    "    model.train() #training mode\n",
    "\n",
    "    for e in range(1, args.epochs+1):\n",
    "        running_loss=0\n",
    "        for ii, (data,target) in enumerate(datasets): #iterates over pointers to remote data\n",
    "            steps+=1\n",
    "            \n",
    "            #FEDERATION STEP\n",
    "            model.send(data.location) #send model to remote worker\n",
    "            \n",
    "            #NB the steps below all happen remotely\n",
    "            optimizer.zero_grad()#zero out gradients so that one forward pass doesnt pick up previous forward's gradients\n",
    "            outputs = model.forward(data) #make prediction\n",
    "            outputs = outputs.reshape(1,-1) #get shape of (1,2) as we need at least two dimension\n",
    "            loss = criterion(outputs,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #FEDERATION STEP\n",
    "            model.get() #get model with new gradients back from remote worker\n",
    "            \n",
    "            #FEDERATION STEP\n",
    "            _loss = loss.get() #get loss from remote worker\n",
    "            running_loss+=_loss\n",
    "            \n",
    "            print_every= 200\n",
    "            if (ii+1) % print_every == 0:\n",
    "                print('Train Epoch: {} [{}/{}]  \\tLoss: {:.6f}'.format(\n",
    "                    e, ii+1, len(datasets), running_loss/print_every))\n",
    "                \n",
    "                running_loss=0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated Training on 653 remote workers (dataowners)\n",
      "Train Epoch: 1 [100/653]  \tLoss: 0.005786\n",
      "Train Epoch: 1 [200/653]  \tLoss: 0.004275\n",
      "Train Epoch: 1 [300/653]  \tLoss: 0.006068\n",
      "Train Epoch: 1 [400/653]  \tLoss: 0.008939\n",
      "Train Epoch: 1 [500/653]  \tLoss: 0.008683\n",
      "Train Epoch: 1 [600/653]  \tLoss: 0.003058\n",
      "CPU times: user 6.17 s, sys: 3.42 ms, total: 6.17 s\n",
      "Wall time: 6.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Model(args)\n",
    "federated_train(model, train_loader, 1, nn.NLLLoss(), opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Viola!_ Now we have a federated model where the data never leaves the ownership of a remote device. We can implement this in a way where each user's device is a worker. The problem that occurs here, is that even though the data never leaves an owner's device, `model.get()` returns a new version of the model, which in turn violates privacy of the data owners by revealing information on their data through the updates that were made to the model. A solution to this problem is to use a **trusted third-party aggregator** to combine the remotely trained models into one, *before* sending it to the end-user (in this case me, the credit provider).\n",
    "\n",
    "Notice how the federated model is about 6.5x slower than the non-federated model. This is simply one of the trade-offs that we have to be willing to make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fl_model_avg\"></a>\n",
    "### Federated Learning with Model Averaging\n",
    "We can perform federated learning in a way that trains a model on the data of each remote worker, and uses a *'trusted aggregator'* to combine the models into one. In this way, the non-trusted party, me for example, cannot tell which remote worker has updated gradients in what way. Gradient updates can be reverse engineered to understand what data has been passed through the network. This is an added layer of privacy protection in federated learning. The downside of this approach, however, is that it requires all parties to trust said aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to use MSELoss because NLLLoss does not work well with federated aggregation\n",
    "# transform the label to handle it better,\n",
    "\n",
    "labels = pd.get_dummies(df['label']).values.astype(float)\n",
    "target = th.tensor(labels, dtype=th.float32,\n",
    "                   requires_grad=False).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def connect_to_workers(n_workers, secure_worker=False):\n",
    "    workers = [sy.VirtualWorker(hook, id=name) for name in df.name.str/\n",
    "                                        .replace(' ', '').values[:n_workers]]\n",
    "\n",
    "    if secure_worker:\n",
    "        return workers, sy.VirtualWorker(hook, id='trusted_aggregator')\n",
    "\n",
    "    else:\n",
    "        return workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 23:31:57.275970 140383773411136 hook.py:98] Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(th)\n",
    "workers, trusted_aggregator = connect_to_workers(len(dataset),\n",
    "                                                 secure_worker=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<VirtualWorker id:PaulinePike #objects:30>,\n",
       " <VirtualWorker id:SharonReed #objects:30>,\n",
       " <VirtualWorker id:AlyshaWalker #objects:30>,\n",
       " <VirtualWorker id:StevenSpeziale #objects:30>,\n",
       " <VirtualWorker id:LouisLembke #objects:30>]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Send Data to Remote Worker\n",
    "In this step we need to send a copy of the model to each remote worker, as well as a new optimizer object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PaulinePike', 'SharonReed', 'AlyshaWalker', 'StevenSpeziale', 'LouisLembke']\n"
     ]
    }
   ],
   "source": [
    "# Send data to remote workers\n",
    "# Cast the result in BaseDatasets\n",
    "remote_dataset_list = []\n",
    "for i in range(len(dataset)):\n",
    "\n",
    "    d, t = data[i], target[i]\n",
    "    # send to worker before adding to dataset\n",
    "    r_d = d.reshape(1, -1).send(workers[i])\n",
    "    r_t = t.reshape(1, -1).send(workers[i])\n",
    "\n",
    "    dtset = sy.BaseDataset(r_d, r_t)\n",
    "    remote_dataset_list.append(dtset)\n",
    "\n",
    "# Build the FederatedDataset object\n",
    "remote_dataset = sy.FederatedDataset(remote_dataset_list)\n",
    "print(remote_dataset.workers[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments(in_size, out_size, hidden_layers, activation=F.softmax, dim=1)\n",
    "# for MSE loss, we want to use softmax and not log_softmax\n",
    "base_model = Model(args)\n",
    "\n",
    "models = [base_model.copy().send(w) for w in workers]\n",
    "optimizers = [optim.SGD(params=m.parameters(), lr=args.lr) for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new training logic to reflect PARALLEL federated learning with trusted aggregator\n",
    "def federated_train_trusted_agg(models, datasets, optimizers):\n",
    "    for e in range(1, args.epochs+1):\n",
    "        running_loss = 0\n",
    "        for i in range(len(models)):  # train each model concurrently\n",
    "            model = models[i]\n",
    "            opt = optimizers[i]\n",
    "            _d = datasets.datasets[model.location.id]  # remote dataset\n",
    "\n",
    "            # NB the steps below all happen remotely\n",
    "            opt.zero_grad()  # zero out gradients so that one forward pass doesnt pick up previous forward's gradients\n",
    "            outputs = model.forward(_d.data)  # make prediction\n",
    "            # get shape of (1,2) as we need at least two dimension\n",
    "            outputs = outputs.reshape(1, -1)\n",
    "            # NllLoss does not work well here...\n",
    "            loss = ((outputs - _d.targets)**2).sum()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # FEDERATION STEP\n",
    "            _loss = loss.get().data  # get loss from remote worker\n",
    "            if th.isnan(_loss) or _loss > 10:\n",
    "                print(model.location.id, outputs.get(), _d.targets.get(), _loss)\n",
    "                continue\n",
    "\n",
    "            running_loss += _loss\n",
    "        print('Epoch: {} \\tLoss: {:.6f}'.format(\n",
    "            e, running_loss/i))\n",
    "\n",
    "    # move trained models to trusted thrid party\n",
    "    for m in models:\n",
    "        m.move(trusted_aggregator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated Training on 653 remote workers with Trusted Aggregator\n",
      "Train Epoch: 1 \tLoss: 0.559299\n",
      "Train Epoch: 2 \tLoss: 0.515398\n",
      "Train Epoch: 3 \tLoss: 0.479686\n",
      "Train Epoch: 4 \tLoss: 0.449864\n",
      "Train Epoch: 5 \tLoss: 0.424561\n"
     ]
    }
   ],
   "source": [
    "federated_train_trusted_agg(models, remote_dataset, optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_avg(base_model, models):\n",
    "    '''\n",
    "    Average weights and biases of models on trusted aggregator\n",
    "\n",
    "    Parameters\n",
    "    ::models - list: pointers to remote models, should be on trusted aggregator\n",
    "\n",
    "    Returns\n",
    "    ::avg_weights\n",
    "    ::avg_bias\n",
    "    '''\n",
    "    # average out each hidden layer individually\n",
    "    for i in range(len(base_model.hidden_layers)):\n",
    "        weights, biases = zip(*[(m.hidden_layers[i].weight.data,\n",
    "                                 m.hidden_layers[i].bias.data) for m in models])\n",
    "        base_model.hidden_layers[i].weight/\n",
    "                                   .set_((sum(weights)/len(models)).get())\n",
    "        base_model.hidden_layers[i].bias.set_((sum(biases)/len(models)).get())\n",
    "\n",
    "    # average out output layer\n",
    "    weights, biases = zip(*[(m.output.weight.data,\n",
    "                             m.output.bias.data) for m in models])\n",
    "    base_model.output.weight.set_((sum(weights)/len(models)).get())\n",
    "    base_model.output.bias.set_((sum(biases)/len(models)).get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the model on trusted aggregator\n",
    "with th.no_grad():\n",
    "    set_model_avg(base_model, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated Training on 653 remote workers with Trusted Aggregator\n",
      "Model Training Iteration 1\n",
      "Federated Training on 653 remote workers with Trusted Aggregator\n",
      "Train Epoch: 1 \tLoss: 0.484676\n",
      "Train Epoch: 2 \tLoss: 0.459549\n",
      "Train Epoch: 3 \tLoss: 0.448780\n",
      "Train Epoch: 4 \tLoss: 0.440922\n",
      "Train Epoch: 5 \tLoss: 0.433375\n",
      "Model Training Iteration 2\n",
      "Federated Training on 653 remote workers with Trusted Aggregator\n",
      "Train Epoch: 1 \tLoss: 0.641504\n",
      "Train Epoch: 2 \tLoss: 0.547193\n",
      "Train Epoch: 3 \tLoss: 0.528542\n",
      "Train Epoch: 4 \tLoss: 0.508749\n",
      "Train Epoch: 5 \tLoss: 0.499939\n",
      "Model Training Iteration 3\n",
      "Federated Training on 653 remote workers with Trusted Aggregator\n",
      "Train Epoch: 1 \tLoss: 0.625306\n",
      "Train Epoch: 2 \tLoss: 0.593438\n",
      "Train Epoch: 3 \tLoss: 0.563196\n",
      "Train Epoch: 4 \tLoss: 0.541335\n",
      "Train Epoch: 5 \tLoss: 0.527835\n",
      "Model Training Iteration 4\n",
      "Federated Training on 653 remote workers with Trusted Aggregator\n",
      "Train Epoch: 1 \tLoss: 0.489112\n",
      "Train Epoch: 2 \tLoss: 0.441764\n",
      "Train Epoch: 3 \tLoss: 0.402509\n",
      "Train Epoch: 4 \tLoss: 0.369080\n",
      "Train Epoch: 5 \tLoss: 0.339746\n",
      "Model Training Iteration 5\n",
      "Federated Training on 653 remote workers with Trusted Aggregator\n",
      "Train Epoch: 1 \tLoss: 0.713599\n",
      "Train Epoch: 2 \tLoss: 0.596377\n",
      "Train Epoch: 3 \tLoss: 0.531746\n",
      "Train Epoch: 4 \tLoss: 0.487387\n",
      "Train Epoch: 5 \tLoss: 0.452840\n"
     ]
    }
   ],
   "source": [
    "# We now have to put the training and averaging steps together, so that we see an improvement in the whole model\n",
    "\n",
    "print((f'Federated Training on {len(remote_dataset)}'\n",
    "       ' remote workers with Trusted Aggregator'))\n",
    "for i in range(1, args.epochs+1):\n",
    "    print(f\"Model Training Iteration {i}\")\n",
    "    base_model = Model(args)\n",
    "\n",
    "    models = [base_model.copy().send(w) for w in workers]\n",
    "    optimizers = [optim.SGD(params=m.parameters(), lr=args.lr) for m in models]\n",
    "\n",
    "    federated_train_trusted_agg(models, remote_dataset, optimizers)\n",
    "\n",
    "    # Average the model on trusted aggregator\n",
    "    with th.no_grad():\n",
    "        set_model_avg(base_model, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not trained a deep learning model using federated learning with a trusted aggregator! Make sure to test the model on a hold-out dataset. For the purpose of these examples, I will exclude testing sets for the sake of time.\n",
    "Nevertheless, this **data is not yet encrypted** and we could deduce things specific to the applicant just by getting or looking at the remote data. <br>\n",
    "In comes **encrypted deep learning**! Here we want to encrypt gradients such that no trusted aggregator is needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"encrypted_dl\"></a>\n",
    "## Encrypted Deep Learning\n",
    "Encrypted Deep Learning aims to preserve model accuracy and predictive power, without compromising the privacy and identity of individual users in the data. Encrypted deep learning provides privacy by enciphering the values that are being computed. Encrypted deep learning can involve encrypting the gradients or encrypting the data as well. I will walk through examples of encrypted deep learning using secure multi-party computation.\n",
    "\n",
    "<a id=\"smpc\"></a>\n",
    "#### Secure Multi-Party Computation (SMPC)\n",
    "PySyft has employed encryption using secure multi-party computation (SMPC). To learn more about the basics of SMPC and differential privacy [check out my SMPC (PySyft inspired) notebook](https://htmlpreview.github.io/?https://github.com/mkucz95/private_ai_finance/blob/master/secure_multi_party_computation.html). This will help you understand how the steps below successfully encrypt data while preserving model accuracy.\n",
    "\n",
    "<a id=\"fl_encrypt_avg\"></a>\n",
    "### Encrypted Gradient Aggregation\n",
    "\n",
    "The previous implementations of federated learning have all relied on a *'trusted aggregator'*. Unfortunately, in many scenarios we would probably not want to have to rely on such a third-party, potentially because no third-party can be deemed trustworthy enough.\n",
    "\n",
    "Encrypted gradient aggregation follows largely the same process that unencrypted federated learning with trusted aggregator does. The difference exists in how training is conducted, since now we employ secure multi-party computation to aggregate the gradients (the gradients are encrypted across multiple workers). Therefore, only the training function changes. Since it is largely the same as the previous step, I won't provide a worked example, however visit [PySyft's tutorial to learn more](https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/Part%2010%20-%20Federated%20Learning%20with%20Secure%20Aggregation.ipynb). To summarize encrypted gradient aggregation, since each remote worker has their own model, encrypting this model includes sharing the parameters (weights and biases of the network) across all the workers. Using SMPC, we can aggregate the encrypted parameters after each remote model has passed through a training run. Since we would only get the aggregated model, we are unable to deduce individual worker's model parameters or gradients, ensuring privacy without the need for a trusted third-party aggregator.\n",
    "\n",
    "Instead, let's work out how to train a network where the data, model parameters, AND the gradients are all encrypted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-to-End Encryption\n",
    "There are certain scenarios where for maximum privacy it is ideal to keep data encrypted as well as keep each federated model encrypted. **end-to-end encryption**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are scenarios in which a model will have already been trained, for example from past customer data (before the implementation of differentially private techniques), or that we want to train a new secure model on entirely encrypted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_provider = sy.VirtualWorker(hook, id='crypto_provider')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `crypto_provider` is needed to provide random numbers and the field quotient `Q` as outlined in the [SMPC tutorial](https://github.com/mkucz95/private_ai_finance/blob/master/secure_multi_party_computation.ipynb). The `crypto_provider` never 'owns' or handles any data, it is simply there to ensure secure computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0000, 30.8300,  0.0000,  1.2500,  0.0000], grad_fn=<SliceBackward>),\n",
       " (Wrapper)>FixedPrecisionTensor>tensor([    0, 30830,     0,  1250,     0]))"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for SMPC we need to work with integers.\n",
    "# Therefore we convert all decimals to integers depending on the precision we want.\n",
    "# this adds some noise/error to the data\n",
    "data[0][:5], data.fix_precision(5)[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# We don't use the whole dataset for efficiency purpose, but feel free to increase these numbers\n",
    "n_train_items = 10  # len(dataset)\n",
    "n_test_items = 10  # len(dataset)\n",
    "\n",
    "\n",
    "def get_private_data_loaders(precision_fractional, workers, crypto_provider):\n",
    "    '''\n",
    "    Encrypt training and test data (both the features and targets)\n",
    "    '''\n",
    "    def secret_share(tensor):\n",
    "        \"\"\"\n",
    "        Transform to fixed precision and secret share a tensor\n",
    "        \"\"\"\n",
    "        return (\n",
    "            tensor\n",
    "            .fix_precision(precision_fractional=precision_fractional)\n",
    "            .share(*workers, crypto_provider=crypto_provider,\n",
    "                   requires_grad=True)\n",
    "        )\n",
    "\n",
    "    private_train_loader = [\n",
    "        (secret_share(data), secret_share(target))\n",
    "        for i, (data, target) in enumerate(dataset)\n",
    "        if i < n_train_items\n",
    "    ]\n",
    "\n",
    "    # TODO iterate on this\n",
    "    private_test_loader = [\n",
    "        (secret_share(data), secret_share(target.float()))\n",
    "        for i, (data, target) in enumerate(dataset)\n",
    "        if i < n_test_items\n",
    "    ]\n",
    "\n",
    "    return private_train_loader, private_test_loader\n",
    "\n",
    "\n",
    "private_train_loader, private_test_loader = get_private_data_loaders(\n",
    "    precision_fractional=args.precision_fractional,\n",
    "    workers=workers,\n",
    "    crypto_provider=crypto_provider\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:14753523827 -> MichaelBerry:81453547002]\n",
       " \t-> [PointerTensor | me:27673415801 -> SaraHoyos:6285248345]\n",
       " \t*crypto provider: crypto_provider*,\n",
       " (Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:98745188905 -> MichaelBerry:63953560831]\n",
       " \t-> [PointerTensor | me:76257879237 -> SaraHoyos:58040980791]\n",
       " \t*crypto provider: crypto_provider*)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "private_train_loader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
      "\t-> [PointerTensor | me:54906963309 -> MichaelBerry:93683502389]\n",
      "\t-> [PointerTensor | me:63042872172 -> SaraHoyos:73389774872]\n",
      "\t-> [PointerTensor | me:48900505983 -> KevinMack:49273037583]\n",
      "\t*crypto provider: crypto_provider* (Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
      "\t-> [PointerTensor | me:23710369807 -> MichaelBerry:54339387032]\n",
      "\t-> [PointerTensor | me:10964909446 -> SaraHoyos:62114046832]\n",
      "\t-> [PointerTensor | me:40775904429 -> KevinMack:1070431350]\n",
      "\t*crypto provider: crypto_provider*\n"
     ]
    }
   ],
   "source": [
    "smpc_remote_dataset = []\n",
    "for i in range(10):\n",
    "    d, t = dataset[i]\n",
    "\n",
    "    # send to worker before adding to dataset\n",
    "    # securely encrypt across all workers\n",
    "    r_d = d.fix_precision().share(*workers, crypto_provider=crypto_provider,\n",
    "                                  requires_grad=True)\n",
    "    r_t = t.fix_precision().share(*workers, crypto_provider=crypto_provider,\n",
    "                                  requires_grad=True)\n",
    "\n",
    "    smpc_remote_dataset.append((r_d, r_t))\n",
    "\n",
    "print(r_d, r_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, that the data now also is also type `AutogradTensor`. As is explained by PySyft, we require the data tensors to maintain gradients, but since we fix the precision and PyTorch's autograd only works on float type tensors, PySyft has a special `AutogradTensor` to compute the gradient graph for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# new training logic to reflect federated learning\n",
    "# generally speaking the training of fully encrypted networks is very similar\n",
    "# to normal training\n",
    "def encrypted_federated_train(model, datasets, optimizer, args):\n",
    "    print(f'SMPC Training on {len(datasets)} remote workers (dataowners)')\n",
    "    steps = 0\n",
    "    model.train()  # training mode\n",
    "    \n",
    "    for e in range(1, args.epochs+1):\n",
    "        running_loss = 0\n",
    "        for ii, (data, target) in enumerate(datasets):  # iterates over pointers to remote data\n",
    "            steps += 1\n",
    "            # TODO model.send()?\n",
    "            # NB the steps below all happen remotely\n",
    "            # zero out gradients so that one forward pass doesnt pick up previous forward's gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model.forward(data)  # make prediction\n",
    "            # get shape of (1,2) as we need at least two dimension\n",
    "            outputs = outputs.reshape(1, -1)\n",
    "            loss = ((outputs - target)**2).sum().refresh()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # get loss from remote worker and unencrypt\n",
    "            _loss = loss.get().float_precision()\n",
    "            running_loss += _loss\n",
    "\n",
    "            print_every = 100\n",
    "            if steps % print_every == 0:\n",
    "                print('Train Epoch: {} [{}/{}]  \\tLoss: {:.6f}'.format(\n",
    "                    e, ii+1, len(datasets), _loss/print_every))\n",
    "\n",
    "                running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments for a MSE Loss Network\n",
    "args = Arguments(in_size, out_size, hidden_layers, activation=F.softmax, dim=1)\n",
    "\n",
    "# create new model, fix the precision and share the gradients across workers\n",
    "smpc_model = Model(args)/\n",
    "            .fix_precision(precision_fractional=args.precision_fractional) /\n",
    "            .share(*workers, crypto_provider=crypto_provider,\n",
    "                   requires_grad=True)\n",
    "\n",
    "smpc_opt = opt.fix_precision(precision_fractional=args.precision_fractional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMPC Training on 10 remote workers (dataowners)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected device cpu and dtype Float but got device cpu and dtype Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-199-583a558f82d9>\u001b[0m in \u001b[0;36mencrypted_federated_train\u001b[0;34m(model, datasets, optimizer, args)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#make prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get shape of (1,2) as we need at least two dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0;31m# Send the new command to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                 \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36m__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__isub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/syft/frameworks/torch/tensors/interpreters/autograd.py\u001b[0m in \u001b[0;36mmethod_with_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 )\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0;31m# Put back SyftTensor on the tensors found in the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/syft/frameworks/torch/overload_torch.py\u001b[0m in \u001b[0;36mhook_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Send it to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# Put back SyftTensor on the tensors found in the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/syft/frameworks/torch/tensors/interpreters/precision.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(self, _self, other)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0m_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m%=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfield\u001b[0m  \u001b[0;31m# Wrap around the field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/syft/frameworks/torch/overload_torch.py\u001b[0m in \u001b[0;36mhook_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Send it to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m# Put back SyftTensor on the tensors found in the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/syft/frameworks/torch/tensors/interpreters/additive_shared.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(self, shares, other)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mnew_shares\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshares\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mnew_shares\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_shares\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_pointer_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, recipient, message, return_ids)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSGTYPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mResponseSignatureError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, msg_type, message, location)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mbin_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# Step 3: deserialize the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_send_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mVirtualWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWorker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFederatedClient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseWorker\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_recv_msg\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mrecv_msg\u001b[0;34m(self, bin_message)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"worker {self} received {sy.codes.code2MSGTYPE[msg_type]} {contents}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# Step 1: route message to appropriate function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_router\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmsg_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;31m# Step 2: Serialize the message to simple python objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mexecute_command\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# TODO Andrew thinks this is gross, please fix. Instead need to properly deserialize strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m                     \u001b[0;31m# we can make some errors more descriptive with this method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mroute_method_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# means that there is a wrapper to remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p_venv/lib/python3.6/site-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m                     \u001b[0;31m# we can make some errors more descriptive with this method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected device cpu and dtype Float but got device cpu and dtype Long"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "encrypted_federated_train(smpc_model, private_train_loader, opt, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Notes\n",
    "\n",
    "**Loss Functions**\n",
    "Using negative log-likelihood loss is not yet supported for multi-party computation. This is due to the nature of computation required for the loss function calculation.\n",
    "\n",
    "_Options_\n",
    "1. train on non-encrypted data (could be differentially private though) and then make predictions using encrypted data. This way we can use NLLLoss for training\n",
    "2. Train the model on federated, encrypted data using mean squared error\n",
    "\n",
    "The type of loss we use [MSELoss](https://pytorch.org/docs/stable/nn.html#mseloss) vs [NLLLoss](https://pytorch.org/docs/stable/nn.html#nllloss) would indicate that we need to handle our target tensors a little differently. These loss functions expect different shapes as the target inputs. Read the documentation if you want to find out more.\n",
    "***\n",
    "**Feature Normalization**<br>\n",
    "Normalization can be handled on a per-datum basis. When working with images, for example, you can pass in normalization parameters before hand, so that each remote worker can normalize their data. However, normalization generally becomes difficult for encrypted data since it is not possible to ensure total privacy. However, data could generally be normalized with such a trusted party, but this introduces inherent privacy problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Even though all the data here is encrypted it does not prevent an adversarial attack where shares are intentionally corrupted during computation. This is generally considered an open problem in SMPC and encrypted deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dp_dl\"></a>\n",
    "## Differential Privacy for Deep Learning\n",
    "Differential privacy techniques provide certain guarantees for privacy in the context of deep learning. Instead of encrypting data, we add noise to the data (local DP) or to the output of a query (global DP) such that privacy is preserved to an acceptable degree. To familiarize yourself with Differential Privacy, visit a short guide I have put together [here](https://htmlpreview.github.io/?https://github.com/mkucz95/private_ai_finance/blob/master/differential-privacy.html). For the purpose of this example, however, I have not implemented differential privacy since data will be encrypted end-to-end anyway. However, one could have private deep learning employing differential privacy on a local or global level, and then work with unencrypted data, gradients, and models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- training and validation set\n",
    "- loss graph\n",
    "\n",
    "- https://github.com/OpenMined/PySyft/tree/dev/examples/tutorials/advanced/websockets-example-MNIST\n",
    "\n",
    "- create final websocket live code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.183px",
    "left": "910px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
