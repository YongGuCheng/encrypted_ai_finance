{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import syft as sy\n",
    "import torch as th\n",
    "from helpers import Model\n",
    "\n",
    "# BEWARE, ignoreing warnings is not always a good idea\n",
    "# I am doing it for presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"federated_dl\"></a>\n",
    "## Federated Deep Learning\n",
    "The idea behind federated learning is that we train a model on subsets of data (encrypted or otherwise) that never leaves the ownership of an individual. In this example of credit rating scores it would allow people to submit claims without ever losing ownership of their data. It requires very little trust of the party to which the application is being submitted.\n",
    "\n",
    "Even though we currently have our dataset located locally, we want to simulate having many people in our network who each maintain ownership of their data. Therefore we have to create a virtual worker for each datum. The work/data flow in this situation would be as follows:\n",
    "\n",
    "- get pointers to training data on each remote worker <br>\n",
    "**Training Steps:**\n",
    "- send model to remote worker\n",
    "- train model on data located with remote worker\n",
    "- receive updated model from remote worker\n",
    "- repeat for all workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load('data/features.npy')\n",
    "labels = np.load('data/labels.npy')\n",
    "data = th.tensor(features, dtype=th.float32, requires_grad=True)\n",
    "target = th.tensor(labels, dtype=th.int64, requires_grad=False).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self, in_size, out_size, hidden_layers,\n",
    "                       activation=F.softmax, dim=-1):\n",
    "        self.batch_size = 1\n",
    "        self.drop_p = None\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.001\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.precision_fractional=10\n",
    "        self.activation = activation\n",
    "        self.dim = dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 22:10:23.372497 140304801736512 hook.py:102] Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(th)\n",
    "\n",
    "def connect_to_workers(n_workers, secure_worker=False):\n",
    "    '''\n",
    "    Connect to remote workers with PySyft\n",
    "    \n",
    "    Inputs\n",
    "        n_workers (int) - how many workers to connect to\n",
    "        secure_worker (bool) - whether to return a trusted aggregator as well\n",
    "        \n",
    "    Outputs\n",
    "        workers (list, <PySyft.VirtualWorker>)\n",
    "    '''\n",
    "    workers = [sy.VirtualWorker(hook, id=f'w_{i}') for i in range(n_workers)]\n",
    "\n",
    "    if secure_worker:\n",
    "        return workers, sy.VirtualWorker(hook, id='trusted_aggregator')\n",
    "\n",
    "    else:\n",
    "        return workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [(data[i], target[i]) for i in range(len(data))]\n",
    "\n",
    "#instantiate model\n",
    "in_size = data[0].shape[0]\n",
    "out_size = 2\n",
    "hidden_layers=[30,15]\n",
    "workers = connect_to_workers(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<VirtualWorker id:w_0 #objects:12>,\n",
       " <VirtualWorker id:w_1 #objects:4>,\n",
       " <VirtualWorker id:w_2 #objects:4>,\n",
       " <VirtualWorker id:w_3 #objects:4>,\n",
       " <VirtualWorker id:w_4 #objects:4>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers[:5] \n",
    "# each individual worker corresponds to a person, or rather their device\n",
    "# currently these people have no objects associated with them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Data to Remote Worker\n",
    "In reality the data of each person would already be on a remote worker. Either each person's device or aggregated into multiple remote workers by a secure third party.\n",
    "\n",
    "Here we have two options:\n",
    "1. send the data to each worker individually\n",
    "2. use PySyft's implementation of PyTorch's `Dataset` and `DataLoader`\n",
    "\n",
    "I will use PySyft's `BaseDataset`, `FederatedDataset` and `FederatedDataLoader` since this simplifies dataprocessing for larger applications, even though it is not necessary for this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[PointerTensor | me:69228889286 -> w_0:35953420875]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 1\n",
    "remote_dataset = []\n",
    "for i in range(len(dataset)):\n",
    "    d, t = dataset[i]\n",
    "    \n",
    "    r_d = d.reshape(1,-1).send(workers[i])\n",
    "    r_t = t.reshape(1,-1).send(workers[i])\n",
    "    \n",
    "    remote_dataset.append((r_d, r_t))\n",
    "    \n",
    "r_d, r_t = remote_dataset[0]\n",
    "r_d #this is now a pointer to remote data rather than an actual tensor on our device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w_0', 'w_1', 'w_2', 'w_3', 'w_4']\n"
     ]
    }
   ],
   "source": [
    "# Option 2\n",
    "# Cast the result in BaseDatasets\n",
    "remote_dataset_list = []\n",
    "for i in range(len(dataset)):\n",
    "    d, t = dataset[i] #get data\n",
    "\n",
    "    #send to worker before adding to dataset\n",
    "    r_d = d.reshape(1,-1).send(workers[i])\n",
    "    r_t = t.reshape(1,-1).send(workers[i])\n",
    "    \n",
    "    dtset = sy.BaseDataset(r_d, r_t)\n",
    "    remote_dataset_list.append(dtset)\n",
    "\n",
    "# Build the FederatedDataset object\n",
    "remote_dataset = sy.FederatedDataset(remote_dataset_list)\n",
    "print(remote_dataset.workers[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = sy.FederatedDataLoader(remote_dataset, batch_size=1,\n",
    "                                      shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#new training logic to reflect federated learning\n",
    "def federated_train(model, datasets, criterion, args):\n",
    "    #use a simple stochastic gradient descent optimizer\n",
    "    #define optimizer for each model\n",
    "    optimizer = optim.SGD(params=model.parameters(), lr=args.lr)\n",
    "    \n",
    "    print(f'Federated Training on {len(datasets)} remote workers (dataowners)')\n",
    "    steps=0\n",
    "    model.train() #training mode\n",
    "\n",
    "    for e in range(1, args.epochs+1):\n",
    "        running_loss=0\n",
    "        for ii, (data,target) in enumerate(datasets): \n",
    "            #iterates over pointers to remote data\n",
    "            steps+=1\n",
    "            \n",
    "            #FEDERATION STEP\n",
    "            model.send(data.location) \n",
    "            #send model to remote worker\n",
    "            \n",
    "            #NB the steps below all happen remotely\n",
    "            optimizer.zero_grad()#zero out gradients so that one forward pass doesnt pick up previous forward's gradients\n",
    "            outputs = model.forward(data) #make prediction\n",
    "            outputs = outputs.reshape(1,-1) #get shape of (1,2) as we need at least two dimension\n",
    " \n",
    "            loss = criterion(outputs,target[0])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #FEDERATION STEP\n",
    "            model.get() #get model with new gradients back from remote worker\n",
    "            \n",
    "            #FEDERATION STEP\n",
    "            _loss = loss.get() #get loss from remote worker\n",
    "            running_loss+=_loss\n",
    "            \n",
    "            print_every= 200\n",
    "            if (ii+1) % print_every == 0:\n",
    "                print('Train Epoch: {} [{}/{}]  \\tLoss: {:.6f}'.format(\n",
    "                    e, ii+1, len(datasets), running_loss/print_every))\n",
    "                \n",
    "                running_loss=0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federated Training on 653 remote workers (dataowners)\n",
      "Train Epoch: 1 [200/653]  \tLoss: 1.117102\n",
      "Train Epoch: 1 [400/653]  \tLoss: 483.192688\n",
      "Train Epoch: 1 [600/653]  \tLoss: 41.180523\n",
      "Train Epoch: 2 [200/653]  \tLoss: 0.591983\n",
      "Train Epoch: 2 [400/653]  \tLoss: 0.804055\n",
      "Train Epoch: 2 [600/653]  \tLoss: 0.728580\n",
      "Train Epoch: 3 [200/653]  \tLoss: 0.627162\n",
      "Train Epoch: 3 [400/653]  \tLoss: 0.756000\n",
      "Train Epoch: 3 [600/653]  \tLoss: 0.709238\n",
      "Train Epoch: 4 [200/653]  \tLoss: 0.657211\n",
      "Train Epoch: 4 [400/653]  \tLoss: 0.724889\n",
      "Train Epoch: 4 [600/653]  \tLoss: 0.698314\n",
      "Train Epoch: 5 [200/653]  \tLoss: 0.681328\n",
      "Train Epoch: 5 [400/653]  \tLoss: 0.704380\n",
      "Train Epoch: 5 [600/653]  \tLoss: 0.692041\n",
      "Train Epoch: 6 [200/653]  \tLoss: 0.699990\n",
      "Train Epoch: 6 [400/653]  \tLoss: 0.690608\n",
      "Train Epoch: 6 [600/653]  \tLoss: 0.688358\n",
      "Train Epoch: 7 [200/653]  \tLoss: 0.714111\n",
      "Train Epoch: 7 [400/653]  \tLoss: 0.681206\n",
      "Train Epoch: 7 [600/653]  \tLoss: 0.686137\n",
      "Train Epoch: 8 [200/653]  \tLoss: 0.724644\n",
      "Train Epoch: 8 [400/653]  \tLoss: 0.674697\n",
      "Train Epoch: 8 [600/653]  \tLoss: 0.684761\n",
      "Train Epoch: 9 [200/653]  \tLoss: 0.732427\n",
      "Train Epoch: 9 [400/653]  \tLoss: 0.670142\n",
      "Train Epoch: 9 [600/653]  \tLoss: 0.683887\n",
      "Train Epoch: 10 [200/653]  \tLoss: 0.738142\n",
      "Train Epoch: 10 [400/653]  \tLoss: 0.666927\n",
      "Train Epoch: 10 [600/653]  \tLoss: 0.683316\n",
      "CPU times: user 59.8 s, sys: 166 ms, total: 59.9 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "args = Arguments(in_size, out_size, hidden_layers, \n",
    "                 activation=F.log_softmax, dim=1)\n",
    "model = Model(args)\n",
    "\n",
    "federated_train(model, train_loader, nn.NLLLoss(), args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Viola!_ Now we have a federated model where the data never leaves the ownership of a remote device. We can implement this in a way where each user's device is a worker. The problem that occurs here, is that even though the data never leaves an owner's device, `model.get()` returns a new version of the model, which in turn violates privacy of the data owners by revealing information on their data through the updates that were made to the model. A solution to this problem is to use a **trusted third-party aggregator** to combine the remotely trained models into one, *before* sending it to the end-user (in this case me, the credit provider).\n",
    "\n",
    "Notice how the federated model is about 6.5x slower than the non-federated model. This is simply one of the trade-offs that we have to be willing to make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in this journey is **Federated Learning with Model Averaging** which you can find [here]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.183px",
    "left": "910px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
