{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0825 12:06:26.303148 140185831077696 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/home/mkucz/p_venv/lib/python3.6/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0825 12:06:26.316387 140185831077696 deprecation_wrapper.py:119] From /home/mkucz/p_venv/lib/python3.6/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import syft as sy\n",
    "import torch as th\n",
    "from helpers import Model, connect_to_workers\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# BEWARE, ignoreing warnings is not always a good idea\n",
    "# I am doing it for presentation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"encrypted_dl\"></a>\n",
    "## Encrypted Deep Learning\n",
    "Encrypted Deep Learning aims to preserve model accuracy and predictive power, without compromising the privacy and identity of individual users in the data. Encrypted deep learning provides privacy by enciphering the values that are being computed. Encrypted deep learning can involve encrypting the gradients or encrypting the data as well. I will walk through examples of encrypted deep learning using secure multi-party computation.\n",
    "\n",
    "<a id=\"smpc\"></a>\n",
    "#### Secure Multi-Party Computation (SMPC)\n",
    "PySyft has employed encryption using secure multi-party computation (SMPC). To learn more about the basics of SMPC and differential privacy [check out my SMPC (PySyft inspired) notebook](https://htmlpreview.github.io/?https://github.com/mkucz95/private_ai_finance/blob/master/secure_multi_party_computation.html). This will help you understand how the steps below successfully encrypt data while preserving model accuracy.\n",
    "\n",
    "<a id=\"fl_encrypt_avg\"></a>\n",
    "### Encrypted Gradient Aggregation\n",
    "\n",
    "The previous implementations of federated learning have all relied on a *'trusted aggregator'*. Unfortunately, in many scenarios we would probably not want to have to rely on such a third-party, potentially because no third-party can be deemed trustworthy enough.\n",
    "\n",
    "Encrypted gradient aggregation follows largely the same process that unencrypted federated learning with trusted aggregator does. The difference exists in how training is conducted, since now we employ secure multi-party computation to aggregate the gradients (the gradients are encrypted across multiple workers). Therefore, only the training function changes. Since it is largely the same as the previous step, I won't provide a worked example, however visit [PySyft's tutorial to learn more](https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/Part%2010%20-%20Federated%20Learning%20with%20Secure%20Aggregation.ipynb). To summarize encrypted gradient aggregation, since each remote worker has their own model, encrypting this model includes sharing the parameters (weights and biases of the network) across all the workers. Using SMPC, we can aggregate the encrypted parameters after each remote model has passed through a training run. Since we would only get the aggregated model, we are unable to deduce individual worker's model parameters or gradients, ensuring privacy without the need for a trusted third-party aggregator.\n",
    "\n",
    "Instead, let's work out how to train a network where the data, model parameters, AND the gradients are all encrypted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load('data/features.npy')\n",
    "labels = np.load('data/labels.npy')\n",
    "data = th.tensor(features, dtype=th.float32, requires_grad=True)\n",
    "target = th.tensor(labels, dtype=th.float32, requires_grad=True).reshape(-1,1)\n",
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self, in_size, out_size, hidden_layers,\n",
    "                       activation=F.softmax, dim=-1):\n",
    "        self.batch_size = 1\n",
    "        self.drop_p = None\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.001\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.precision_fractional=3\n",
    "        self.activation = activation\n",
    "        self.dim = dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [(data[i], target[i]) for i in range(len(data))]\n",
    "\n",
    "#instantiate model\n",
    "in_size = data[0].shape[0]\n",
    "out_size = 1\n",
    "hidden_layers=[30,15]\n",
    "\n",
    "args = Arguments(in_size, out_size, hidden_layers, activation=None)\n",
    "#PyTorch's softmax activation only works with floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please Note* that PyTorch's Softmax activation function only works with float values. However float values are incompatible with SMPC, especially since we have to fix the precision before encrypting. Therefore we have to use an alternate approach to calculating loss, without an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-to-End Encryption\n",
    "There are certain scenarios where for maximum privacy it is ideal to keep data encrypted as well as keep each federated model encrypted. **end-to-end encryption**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are scenarios in which a model will have already been trained, for example from past customer data (before the implementation of differentially private techniques), or that we want to train a new secure model on entirely encrypted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = connect_to_workers(2, hook, secure_worker=False)\n",
    "crypto_provider = sy.VirtualWorker(hook, id='crypto_provider')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `crypto_provider` is needed to provide random numbers and the field quotient `Q` as outlined in the [SMPC tutorial](https://github.com/mkucz95/private_ai_finance/blob/master/secure_multi_party_computation.ipynb). The `crypto_provider` never 'owns' or handles any data, it is simply there to ensure secure computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor \n",
      " tensor([0.0000, 0.2711, 0.0000, 0.0439, 0.0000], grad_fn=<SliceBackward>)\n",
      "\n",
      " Fixed Precision Tensor\n",
      " (Wrapper)>FixedPrecisionTensor>tensor([  0, 271,   0,  43,   0])\n"
     ]
    }
   ],
   "source": [
    "# for SMPC we need to work with integers.\n",
    "# Therefore we convert all decimals to integers depending on the precision we want.\n",
    "# this adds some noise/error to the data\n",
    "print(\"Original Tensor \\n\", data[0][:5])\n",
    "print(\"\\n Fixed Precision Tensor\\n\",data.fix_precision(5)[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "n_train_items = int(len(dataset)*0.7)\n",
    "n_test_items = len(dataset) - n_train_items\n",
    "\n",
    "def get_private_data_loaders(dataset, precision_fractional,\n",
    "                             workers, crypto_provider):\n",
    "    '''\n",
    "    Encrypt training and test data (both the features and targets)\n",
    "    '''\n",
    "    \n",
    "    def secret_share(tensor):\n",
    "        \"\"\"\n",
    "        Transform to fixed precision and secret share a tensor\n",
    "        \"\"\"\n",
    "        return (\n",
    "            tensor\n",
    "            .fix_precision(precision_fractional=precision_fractional)\n",
    "            .share(*workers, crypto_provider=crypto_provider,\n",
    "                   requires_grad=True)\n",
    "        )\n",
    "\n",
    "    private_train_loader = [\n",
    "        (secret_share(data), secret_share(target.reshape(1,1)))\n",
    "        for i, (data, target) in enumerate(dataset)\n",
    "        if i < n_train_items\n",
    "    ]\n",
    "    \n",
    "    private_test_loader = [\n",
    "        (secret_share(data), secret_share(target.float()))\n",
    "        for i, (data, target) in enumerate(dataset[n_train_items:])\n",
    "        if i < n_test_items\n",
    "    ]\n",
    "\n",
    "    return private_train_loader, private_test_loader\n",
    "\n",
    "private_train_loader, private_test_loader = get_private_data_loaders(\n",
    "    dataset,\n",
    "    precision_fractional=args.precision_fractional,\n",
    "    workers=workers,\n",
    "    crypto_provider=crypto_provider\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, that the data now also is also type `AutogradTensor`. As is explained by PySyft, we require the data tensors to maintain gradients, but since we fix the precision and PyTorch's autograd only works on float type tensors, PySyft has a special `AutogradTensor` to compute the gradient graph for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# new training logic to reflect federated learning\n",
    "# generally speaking the training of fully encrypted networks is very similar\n",
    "# to normal training\n",
    "\n",
    "def encrypted_federated_train(model, datasets, optimizer, args):\n",
    "    print(f'SMPC Training on {len(workers)} remote workers (dataowners)')\n",
    "    steps = 0\n",
    "    model.train()  # training mode\n",
    "\n",
    "    for e in tqdm_notebook(range(1, args.epochs+1), unit='epoch',desc='epochs'):\n",
    "        running_loss = 0\n",
    "        for ii, (data, target) in tqdm_notebook(enumerate(datasets),\n",
    "                                                unit='data', desc='data_loop',\n",
    "                                                leave=False,\n",
    "                                                total=len(datasets)):\n",
    "            # iterates over pointers to remote data\n",
    "            #sys.exit()\n",
    "            steps += 1\n",
    "            # NB the steps below all happen remotely\n",
    "            # zero out gradients so that one forward pass doesnt pick up\n",
    "            # previous forward's gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model.forward(data)  # make prediction\n",
    "            # get shape of (1,2) as we need at least two dimension\n",
    "            outputs = outputs.reshape(1, -1)\n",
    "            #MSELoss\n",
    "            loss = ((outputs - target)**2).sum().refresh()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # get loss from remote worker and unencrypt\n",
    "            _loss = loss.get().float_precision().item()\n",
    "            #print(_loss)\n",
    "            #print(_loss.item())\n",
    "            f = outputs-target\n",
    "            #print(outputs.get().float_precision())\n",
    "            #print(target.get().float_precision())\n",
    "\n",
    "            running_loss += _loss\n",
    "        print('Train Epoch: {} \\tLoss: {:.6f}'\\\n",
    "                                      .format(e, running_loss/len(dataset)))\n",
    "        running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate model with fixed precision, and share the model across workers\n",
    "\n",
    "smpc_model = Model(args)\n",
    "smpc_model = smpc_model\\\n",
    "                 .fix_precision(precision_fractional=args.precision_fractional)\\\n",
    "                 .share(*workers, crypto_provider=crypto_provider,\n",
    "                        requires_grad=True)\n",
    "\n",
    "smpc_opt = optim.SGD(params=smpc_model.parameters(), lr=args.lr)\\\n",
    "                .fix_precision(precision_fractional=args.precision_fractional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMPC Training on 2 remote workers (dataowners)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e82538db06f4193b849f3a1a60fdd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epochs', max=10, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca769506b3364ca7802593d3ee79edc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='data_loop', max=457, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "encrypted_federated_train(smpc_model, private_train_loader, smpc_opt, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new training logic to reflect federated learning\n",
    "# generally speaking the training of fully encrypted networks is very similar\n",
    "# to normal training\n",
    "\n",
    "def encrypted_federated_test(model, datasets, args):\n",
    "    print(f'SMPC Testing on {len(workers)} remote workers (dataowners)')\n",
    "    steps = 0\n",
    "    model.eval()  # training mode\n",
    "\n",
    "    pred = []\n",
    "    true = []\n",
    "    running_loss=0.\n",
    "    for ii, (data, target) in tqdm_notebook(enumerate(datasets),\n",
    "                                            unit='datum', desc='testing',\n",
    "                                            total=len(datasets)):\n",
    "        # iterates over pointers to remote data\n",
    "        steps += 1\n",
    "        # NB the steps below all happen remotely\n",
    "        # zero out gradients so that one forward pass doesnt pick up\n",
    "        # previous forward's gradients\n",
    "        outputs = model.forward(data)  # make prediction\n",
    "        # get shape of (1,2) as we need at least two dimension\n",
    "        outputs = outputs.reshape(1, -1)\n",
    "        #MSELoss\n",
    "        loss = ((outputs - target)**2).sum().refresh()\n",
    "\n",
    "        pred.append(outputs.copy().get().float_precision().round().item())\n",
    "        true.append(target.copy().get().float_precision().round().item())\n",
    "\n",
    "        # get loss from remote worker and unencrypt\n",
    "        _loss = loss.get().float_precision().item()\n",
    "        f = outputs-target\n",
    "\n",
    "        running_loss += _loss\n",
    "    print('Testing Loss: {:.6f}'.format(running_loss/len(dataset)))\n",
    "    \n",
    "    return pred, true\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = encrypted_federated_test(smpc_model,\n",
    "                                          private_test_loader, args )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_mtx = confusion_matrix(y_pred, y_true).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_kws = {'size':16}\n",
    "yticks = [0.5,1.5]\n",
    "ax=sns.heatmap(pd.DataFrame(cnf_mtx, columns=['fail', 'credit'],\n",
    "                            index=['fail', 'credit']),\n",
    "               annot=True,annot_kws=annot_kws)\n",
    "ax.set_xlabel('True')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.yticks(va=\"center\")\n",
    "pos, _ = plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = cnf_mtx[1][1]\n",
    "tn = cnf_mtx[0][0]\n",
    "total = cnf_mtx.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"accuracy: {(tp+tn)/total:.2f}\")\n",
    "print(f\"recall: {tp/sum(y_true):.2f}\")\n",
    "print(f\"precision: {tp/sum(y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Notes\n",
    "\n",
    "**Loss Functions**\n",
    "Using negative log-likelihood loss is not yet supported for multi-party computation. This is due to the nature of computation required for the loss function calculation.\n",
    "\n",
    "_Options_\n",
    "1. train on non-encrypted data (could be differentially private though) and then make predictions using encrypted data. This way we can use NLLLoss for training\n",
    "2. Train the model on federated, encrypted data using mean squared error\n",
    "\n",
    "The type of loss we use [MSELoss](https://pytorch.org/docs/stable/nn.html#mseloss) vs [NLLLoss](https://pytorch.org/docs/stable/nn.html#nllloss) would indicate that we need to handle our target tensors a little differently. These loss functions expect different shapes as the target inputs. Read the documentation if you want to find out more.\n",
    "***\n",
    "**Feature Normalization**<br>\n",
    "Normalization can be handled on a per-datum basis. When working with images, for example, you can pass in normalization parameters before hand, so that each remote worker can normalize their data. However, normalization generally becomes difficult for encrypted data since it is not possible to ensure total privacy. However, data could generally be normalized with such a trusted party, but this introduces inherent privacy problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Even though all the data here is encrypted it does not prevent an adversarial attack where shares are intentionally corrupted during computation. This is generally considered an open problem in SMPC and encrypted deep learning.\n",
    "\n",
    "<a id=\"dp_dl\"></a>\n",
    "#### Differential Privacy for Deep Learning\n",
    "Differential privacy techniques provide certain guarantees for privacy in the context of deep learning. Instead of encrypting data, we add noise to the data (local DP) or to the output of a query (global DP) such that privacy is preserved to an acceptable degree. To familiarize yourself with Differential Privacy, visit a short guide I have put together [here](https://htmlpreview.github.io/?https://github.com/mkucz95/private_ai_finance/blob/master/differential-privacy.html). For the purpose of this example, however, I have not implemented differential privacy since data will be encrypted end-to-end anyway. However, one could have private deep learning employing differential privacy on a local or global level, and then work with unencrypted data, gradients, and models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.183px",
    "left": "910px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
